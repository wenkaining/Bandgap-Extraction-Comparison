{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = '/your/HOME/directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Prepare Data\n",
    "\n",
    "Pre-processing steps for preparing the dataset of 200 papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Prepare 200 Papers\n",
    "\n",
    "Steps to prepare the 200-paper dataset:\n",
    "\n",
    "1. Download the arXiv dataset JSON file from <https://www.kaggle.com/datasets/Cornell-University/arxiv?resource=download> and move to `HOME` directory\n",
    "2. Run the code below to generate `ARXIV_JSON` containing 200 randomly selected papers from the materials science domain\n",
    "3. Download the 200 papers' PDF files\n",
    "4. Manually extract bandgap data from the 200 papers and save results to `MANUAL_XLSX` (see example in `manual_pub.xlsx`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "KAGGLE_JSON = os.path.join(HOME, \"arxiv-metadata-oai-snapshot.json\")\n",
    "ARXIV_DIR = os.path.join(HOME, \"arXiv_mtrl-sci\")\n",
    "os.makedirs(ARXIV_DIR, exist_ok=True)\n",
    "ARXIV_CSV = os.path.join(HOME, \"arXiv_mtrl-sci.csv\")\n",
    "ARXIV_JSON = os.path.join(HOME, \"arXiv_mtrl-sci_200.json\")\n",
    "\n",
    "# Define date range for filtering arXiv papers\n",
    "START_DATE = datetime(2000, 1, 1)\n",
    "END_DATE = datetime(2024, 10, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter arXiv papers in the materials science category\n",
    "grouped_data = defaultdict(list)\n",
    "\n",
    "# Read JSON file line by line\n",
    "with open(KAGGLE_JSON, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line.strip())\n",
    "        \n",
    "        # Filter: categories must contain 'mtrl-sci'\n",
    "        if 'mtrl-sci' in entry.get('categories', ''):\n",
    "            # Extract required fields\n",
    "            item = {\n",
    "                'id': entry['id'],\n",
    "                'doi': entry.get('doi', None),\n",
    "                'categories': entry['categories']\n",
    "            }\n",
    "            # Extract and format the creation date of the first version\n",
    "            first_version = entry['versions'][0]['created']\n",
    "            date_v1 = datetime.strptime(first_version, '%a, %d %b %Y %H:%M:%S %Z')\n",
    "            item['date-v1'] = date_v1.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Check if date is within specified range\n",
    "            if START_DATE <= date_v1 <= END_DATE:\n",
    "                # Group by year-month\n",
    "                year_month = date_v1.strftime('%Y-%m')\n",
    "                grouped_data[year_month].append(item)\n",
    "\n",
    "# Write grouped data to JSON files\n",
    "for year_month, items in grouped_data.items():\n",
    "    # Include entry count in filename\n",
    "    count = len(items)\n",
    "    output_file = os.path.join(ARXIV_DIR, f\"{year_month}({count}).json\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        json.dump(items, out_f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 200 papers from filtered results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read JSON files from ARXIV_DIR\n",
    "json_files = sorted(glob(os.path.join(ARXIV_DIR, '*.json')))\n",
    "\n",
    "data = []\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as f:\n",
    "        content = json.load(f)\n",
    "        for paper in content:\n",
    "            # Extract year from date\n",
    "            year = paper['date-v1'][:4] if paper['date-v1'] else None\n",
    "            data.append({\n",
    "            'year': year,\n",
    "            'id': paper['id'],\n",
    "            'doi': paper['doi']\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Count unique IDs and DOIs\n",
    "unique_ids = df['id'].nunique()\n",
    "unique_dois = df['doi'].dropna().nunique()  # Exclude None values\n",
    "\n",
    "print(f\"Unique article IDs: {unique_ids}\")\n",
    "print(f\"Unique DOIs: {unique_dois}\")\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(ARXIV_CSV, index=False)\n",
    "print(f\"\\nData saved to: {ARXIV_CSV}\")\n",
    "\n",
    "# Randomly select 200 papers (only those with both ID and DOI)\n",
    "df_complete = df.dropna(subset=['id', 'doi'])\n",
    "df_sample = df_complete.sample(n=200, random_state=42)\n",
    "df_sample.to_json(ARXIV_JSON, orient='records', indent=2)\n",
    "print(f\"200 paper sample saved to: {ARXIV_JSON}\")\n",
    "\n",
    "# Analyze year distribution\n",
    "year_distribution = df_sample['year'].value_counts().sort_index()\n",
    "print(\"\\nYear distribution:\")\n",
    "print(year_distribution)\n",
    "\n",
    "# Visualize year distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "year_distribution.plot(kind='bar')\n",
    "plt.title('Sample Articles Year Distribution')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parse PDF to TXT\n",
    "\n",
    "Steps to convert PDF files to sentence-segmented text files:\n",
    "\n",
    "1. Download the 200 papers' PDF files to `PDF_DIR`\n",
    "2. Run the code below to generate `TXT_DIR_2` (text files with papers segmented into sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "\n",
    "PDF_DIR = os.path.join(HOME, \"PDF\")\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "TXT_DIR_1 = os.path.join(HOME, \"TXT(fromPDF)\")\n",
    "os.makedirs(TXT_DIR_1, exist_ok=True)\n",
    "TXT_DIR_2 = os.path.join(HOME, \"TXT(fromPDF_processed)\")\n",
    "os.makedirs(TXT_DIR_2, exist_ok=True)\n",
    "\n",
    "# ========== Step 1: Parse PDF to raw text ==========\n",
    "# Extract text from each PDF and save to TXT_DIR_1\n",
    "pdf_files = sorted(glob.glob(os.path.join(PDF_DIR, \"*.pdf\")))\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyMuPDFLoader(pdf_file)\n",
    "    docs = loader.load()\n",
    "    text = \"\".join(doc.page_content for doc in docs)\n",
    "    output_path = os.path.join(TXT_DIR_1, os.path.basename(pdf_file).replace('.pdf', '.txt'))\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "# ========== Step 2: Process text into properly segmented sentences ==========\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# Constants for sentence processing\n",
    "EXCLUDED_ENDINGS = ('Fig.', 'Eq.', 'Figs.', 'et al.')\n",
    "SENTENCE_ENDINGS = ('.', '!', '?')\n",
    "MIN_SENTENCE_LENGTH = 20\n",
    "\n",
    "def should_merge(sentence: str) -> bool:\n",
    "    \"\"\"Determine if the next sentence should be merged with the current one.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Current sentence text\n",
    "        \n",
    "    Returns:\n",
    "        True if the sentence should be merged with the next one\n",
    "    \"\"\"\n",
    "    if not sentence:\n",
    "        return False\n",
    "    # Merge if sentence doesn't end with standard punctuation or ends with excluded patterns\n",
    "    return (not sentence.endswith(SENTENCE_ENDINGS) \n",
    "            or any(sentence.endswith(end) for end in EXCLUDED_ENDINGS))\n",
    "\n",
    "def merge_consecutive(sentences: list[str]) -> list[str]:\n",
    "    \"\"\"Merge consecutive sentences that need to be connected.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentence strings\n",
    "        \n",
    "    Returns:\n",
    "        List of merged sentences\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i, n = 0, len(sentences)\n",
    "    \n",
    "    while i < n:\n",
    "        current = sentences[i].strip()\n",
    "        j = i + 1\n",
    "        \n",
    "        # Continuously merge sentences that need connection\n",
    "        while j < n and should_merge(current):\n",
    "            current += \" \" + sentences[j].strip()\n",
    "            j += 1\n",
    "        \n",
    "        merged.append(current)\n",
    "        i = j\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def merge_short_sentences(sentences: list[str]) -> list[str]:\n",
    "    \"\"\"Merge short sentences into the previous sentence.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentence strings\n",
    "        \n",
    "    Returns:\n",
    "        List with short sentences merged\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    for sentence in sentences:\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # Merge short sentences to previous sentence if available\n",
    "        if merged and len(sentence) < MIN_SENTENCE_LENGTH:\n",
    "            merged[-1] += \" \" + sentence\n",
    "        else:\n",
    "            merged.append(sentence)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def process_txt(input_path: str, output_path: str) -> None:\n",
    "    \"\"\"Process text file to create properly segmented sentences.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input text file\n",
    "        output_path: Path to output processed text file\n",
    "    \"\"\"\n",
    "    # Read and preprocess text\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_text = file.read().replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Initial sentence segmentation\n",
    "    doc = nlp(raw_text)\n",
    "    initial_sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    # Multi-stage processing pipeline\n",
    "    processed = merge_consecutive(initial_sentences)\n",
    "    processed = merge_short_sentences(processed)\n",
    "    processed = merge_consecutive(processed)  # Handle new cases after short sentence merging\n",
    "    \n",
    "    # Write results\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(processed))\n",
    "\n",
    "# Process all txt files from TXT_DIR_1 and save to TXT_DIR_2\n",
    "for txt_file in glob.glob(os.path.join(TXT_DIR_1, \"*.txt\")):\n",
    "    process_txt(txt_file, os.path.join(TXT_DIR_2, os.path.basename(txt_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Extract Bandgap Data\n",
    "\n",
    "This section applies multiple extraction methods to extract bandgap data from the 200 papers.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running extraction methods, download the following projects and models:\n",
    "\n",
    "### Projects\n",
    "1. BandgapDatabase: <https://github.com/QingyangDong-qd220/BandgapDatabase1>\n",
    "2. BERT-PSIE-TC: <https://github.com/StefanoSanvitoGroup/BERT-PSIE-TC>\n",
    "\n",
    "### Language Models & Embeddings\n",
    "1. MatSciBERT: <https://huggingface.co/m3rg-iitd/matscibert> (version: 24a4e4318dda9bc18bff5e6a45debdcb3e1780e3)\n",
    "2. nomic-embed-text: <https://ollama.com/library/nomic-embed-text:latest> (version: 0a109f422b47)\n",
    "   - Install via: `ollama pull nomic-embed-text:latest`\n",
    "3. bge-m3: <https://ollama.com/library/bge-m3:latest> (version: 790764642607)\n",
    "   - Install via: `ollama pull bge-m3:latest`\n",
    "4. Llama2 13B: <https://ollama.com/library/llama2:13b> (version: d475bf4c50bc)\n",
    "   - Install via: `ollama pull llama2:13b`\n",
    "5. Llama 3.1 Nemotron 70B: <https://huggingface.co/bartowski/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF> (version: dfc9cf0b496aea479874ddce703154f07d22ec3d)\n",
    "   - Install via: `ollama create llama3.1:70b -f Modelfile`\n",
    "6. Qwen2.5 14B: <https://ollama.com/library/qwen2.5:14b> (version: 7cdf5a0187d5)\n",
    "   - Install via: `ollama pull qwen2.5:14b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ChemDataExtractor (CDE)\n",
    "\n",
    "**Running Environment:** Docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Docker Setup Note:**\n",
    "\n",
    "```bash\n",
    "docker run --name cde \\\n",
    "  --mount type=bind,source='/your/HOME/directory',target='/home/chemdataextractor2' \\\n",
    "  -it -p 8888:8888 \\\n",
    "  --entrypoint bash obrink/chemdataextractor:2.1.2\n",
    "```\n",
    "\n",
    "If `chemdataextractor2` cannot be imported, rename `/usr/local/lib/python3.8/site-packages/chemdataextractor` to `chemdataextractor2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playsound openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from chemdataextractor2.relex import Snowball\n",
    "from chemdataextractor2.model.units.energy import EnergyModel\n",
    "from chemdataextractor2.model import BaseModel, StringType, ListType, ModelType, Compound\n",
    "from chemdataextractor2.parse import R, I, W, Optional, merge, join, AutoSentenceParser\n",
    "from chemdataextractor2.doc import Sentence, Document\n",
    "\n",
    "class BandGap(EnergyModel):\n",
    "    \"\"\"Custom BandGap model for ChemDataExtractor.\"\"\"\n",
    "    specifier_expression = (\n",
    "        (I(\"band\") + R(\"gaps?\")) | I(\"bandgap\") | I(\"band-gap\") | I(\"Eg\")\n",
    "    ).add_action(join)\n",
    "    specifier = StringType(\n",
    "        parse_expression=specifier_expression, required=True, updatable=True\n",
    "    )\n",
    "    compound = ModelType(\n",
    "        Compound, required=True, contextual=True, binding=True, updatable=False\n",
    "    )\n",
    "    parsers = [AutoSentenceParser()]\n",
    "\n",
    "def run(file_path, article):\n",
    "    \"\"\"Extract bandgap data from a single article using Snowball and AutoSentenceParser.\n",
    "    \n",
    "    This function applies two extraction methods:\n",
    "    1. AutoSentenceParser: Rule-based extraction requiring both value and material name\n",
    "    2. Snowball: Relation extraction model with two similarity thresholds (0.85, then 0.65)\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the text file\n",
    "        article: Article identifier (filename)\n",
    "        \n",
    "    Returns:\n",
    "        List of extracted bandgap records with metadata\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Load document\n",
    "    try:\n",
    "        d = Document.from_file(file_path)\n",
    "    except:\n",
    "        print(\"Unable to read document\")\n",
    "        return\n",
    "\n",
    "    publisher = \"arXiv\"\n",
    "\n",
    "    # Process each paragraph and sentence\n",
    "    for p in d.paragraphs:\n",
    "        for s in p.sentences:\n",
    "            # Skip overly long sentences (likely parsing errors)\n",
    "            if s.end - s.start > 300:\n",
    "                continue\n",
    "\n",
    "            results_snow = []\n",
    "            results_auto = []\n",
    "            snow_85 = False\n",
    "\n",
    "            # Method 1: AutoSentenceParser extraction\n",
    "            BandGap.parsers = [AutoSentenceParser()]\n",
    "            s.models = [BandGap]\n",
    "            auto = s.records.serialize()\n",
    "            \n",
    "            for i in auto:\n",
    "                if \"BandGap\" in i.keys():\n",
    "                    # Require both bandgap value and material name\n",
    "                    if (\"raw_value\" in i[\"BandGap\"].keys() and \n",
    "                        \"compound\" in i[\"BandGap\"].keys()):\n",
    "                        if \"names\" in i[\"BandGap\"][\"compound\"][\"Compound\"].keys():\n",
    "                            i[\"BandGap\"][\"text\"] = s.text\n",
    "                            i['BandGap']['doi'] = article.replace('_', '/').replace('.html', '').replace('.xml', '').replace('.txt', '')\n",
    "                            results_auto.append(i)\n",
    "\n",
    "            # Method 2: Snowball extraction (try with similarity=0.85 first)\n",
    "            snowball.minimum_cluster_similarity_score = 0.85\n",
    "            BandGap.parsers = [snowball]\n",
    "            s.models = [BandGap]\n",
    "            snow = s.records.serialize()\n",
    "            \n",
    "            for i in snow:\n",
    "                if \"BandGap\" in i.keys():\n",
    "                    snow_85 = True\n",
    "                    i[\"BandGap\"][\"text\"] = s.text\n",
    "                    i['BandGap']['doi'] = article.replace('_', '/').replace('.html', '').replace('.xml', '').replace('.txt', '')\n",
    "                    results_snow.append(i)\n",
    "\n",
    "            # If Snowball with 0.85 similarity found nothing, retry with 0.65\n",
    "            if snow_85 == False:\n",
    "                snowball.minimum_cluster_similarity_score = 0.65\n",
    "                BandGap.parsers = [snowball]\n",
    "                s.models = [BandGap]\n",
    "                snow = s.records.serialize()\n",
    "                for i in snow:\n",
    "                    if \"BandGap\" in i.keys():\n",
    "                        i[\"BandGap\"][\"text\"] = s.text\n",
    "                        i['BandGap']['doi'] = article.replace('_', '/').replace('.html', '').replace('.xml', '').replace('.txt', '')\n",
    "                        results_snow.append(i)\n",
    "\n",
    "            # Merge Snowball results into AutoSentenceParser results\n",
    "            for i in results_auto:\n",
    "                i[\"BandGap\"][\"AutoSentenceParser\"] = 1\n",
    "                i[\"BandGap\"][\"Snowball\"] = 0\n",
    "                for j in range(len(results_snow)):\n",
    "                    # Match by material name\n",
    "                    if i['BandGap']['compound']['Compound']['names'] == results_snow[j]['BandGap']['compound']['Compound']['names']:\n",
    "                        i[\"BandGap\"] = results_snow[j][\"BandGap\"]\n",
    "                        i[\"BandGap\"][\"Snowball\"] = 1\n",
    "                        i[\"BandGap\"][\"AutoSentenceParser\"] = 1\n",
    "                        results_snow[j][\"BandGap\"][\"match\"] = 1  # Mark as matched\n",
    "                        continue\n",
    "\n",
    "            # Add unmatched Snowball-only results\n",
    "            for x in results_snow:\n",
    "                if \"match\" not in x[\"BandGap\"].keys():\n",
    "                    x[\"BandGap\"][\"Snowball\"] = 1\n",
    "                    x[\"BandGap\"][\"AutoSentenceParser\"] = 0\n",
    "                    results_auto.append(x)\n",
    "\n",
    "            # Add publisher metadata\n",
    "            if results_auto:\n",
    "                for i in results_auto:\n",
    "                    i[\"BandGap\"][\"publisher\"] = publisher\n",
    "                    results.append(i)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Configuration\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "HOME_DOCKER = \"/home/chemdataextractor2\"\n",
    "PROJECT_DIR = os.path.join(HOME_DOCKER, \"project\")\n",
    "TXT_DIR = os.path.join(HOME_DOCKER, \"TXT(fromPDF_processed)\")\n",
    "OUTPUT_DIR = os.path.join(HOME_DOCKER, \"output\", \"1-ChemDataExtractor\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "TEMP_SAVE = os.path.join(OUTPUT_DIR, \"records_general.joblib\")\n",
    "RUNTIME = os.path.join(OUTPUT_DIR, f'runtime_{dtime}.txt')\n",
    "\n",
    "SNOWBALL_PATH = os.path.join(PROJECT_DIR, \"BandgapDatabase1-main\", \"Snowball_model\", \"general.pkl\")\n",
    "\n",
    "# Load Snowball model\n",
    "snowball = Snowball.load(SNOWBALL_PATH)\n",
    "snowball.minimum_relation_confidence = 0.001\n",
    "snowball.max_candidate_combinations = 100\n",
    "snowball.save_file_name = \"general\"\n",
    "snowball.set_learning_rate(0.0)\n",
    "\n",
    "# Load existing records if available\n",
    "try:\n",
    "    records = joblib.load(TEMP_SAVE)\n",
    "    print(\"Loaded existing records\")\n",
    "except:\n",
    "    records = []\n",
    "    print(\"No existing records found\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Process all text files\n",
    "for file_name in tqdm(os.listdir(TXT_DIR), desc=\"Processing files\"):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(TXT_DIR, file_name)\n",
    "        temp = run(file_path, file_name)\n",
    "        # Save records incrementally\n",
    "        if temp:\n",
    "            pprint(temp)\n",
    "            for record in temp:\n",
    "                records.append(record)\n",
    "            joblib.dump(records, TEMP_SAVE)\n",
    "\n",
    "end_time = datetime.now()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "with open(RUNTIME, 'w') as f:\n",
    "    f.write(f'Total runtime: {run_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 BERT-PSIE (PSIE)\n",
    "\n",
    "**Kernel:** lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_VERSION = \"/your/directory/to/model_cache/huggingface/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import BertTokenizerFast\n",
    "from seqeval.metrics import classification_report\n",
    "import nltk\n",
    "import re\n",
    "from pymatgen.core import Composition\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATA_DIR = os.path.join(HOME, \"TXT(fromPDF_processed)\")\n",
    "\n",
    "# Model paths\n",
    "PROJECT_DIR = os.path.join(HOME, \"project\")\n",
    "PSIE_DIR = os.path.join(PROJECT_DIR, \"BERT-PSIE-TC-main\", \"workflow\")\n",
    "import sys\n",
    "sys.path.insert(1, PSIE_DIR)\n",
    "import psie\n",
    "MODEL_DIR = os.path.join(PSIE_DIR, \"models\", \"Gap\")\n",
    "CLASSIFIER_PATH = os.path.join(MODEL_DIR, \"classifier.pt\")\n",
    "NER_PATH = os.path.join(MODEL_DIR, \"ner\")\n",
    "RELATION_PATH = os.path.join(MODEL_DIR, \"relation\")\n",
    "MAX_LEN = 256  # Maximum sequence length\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = os.path.join(HOME, \"output\", \"2-BERT-PSIE\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "SENTENCES_JSON = os.path.join(OUTPUT_DIR, \"sentences.json\")\n",
    "OUTPUT_JSON_1 = os.path.join(OUTPUT_DIR, \"1-relevant_sentences.json\")\n",
    "OUTPUT_JSON_2_M = os.path.join(OUTPUT_DIR, \"2-test_extraction_multiple_mentions.json\")\n",
    "OUTPUT_CSV_2_S = os.path.join(OUTPUT_DIR, \"2-test_extraction_single_mentions.csv\")\n",
    "OUTPUT_CSV_3 = os.path.join(OUTPUT_DIR, \"3-relations_extraction.csv\")\n",
    "RUNTIME = os.path.join(OUTPUT_DIR, f'runtime_{dtime}.txt')\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "start_time_total = time.time()\n",
    "\n",
    "# ========== Prepare sentences from text files ==========\n",
    "def process_txt_files(data_dir):\n",
    "    \"\"\"Process text files and generate a list of sentences with their DOI sources.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing text files\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with 'sentence' and 'source' (DOI) keys\n",
    "    \"\"\"\n",
    "    sentences_list = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            doi = filename[:-4]  # Remove .txt extension\n",
    "            with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    sentence = line.strip()\n",
    "                    if sentence:\n",
    "                        sentences_list.append({\"sentence\": sentence, \"source\": doi})\n",
    "    return sentences_list\n",
    "\n",
    "# Process text files and save to JSON\n",
    "sentences = process_txt_files(DATA_DIR)\n",
    "with open(SENTENCES_JSON, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(sentences, json_file, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved {len(sentences)} sentences to {SENTENCES_JSON}\")\n",
    "\n",
    "# ========== Stage 1/3: Sentence Classification ==========\n",
    "start_time_classifier = time.time()\n",
    "\n",
    "# Tokenize sentences\n",
    "dataset = load_dataset(path=\"json\", data_files=SENTENCES_JSON, split=\"train\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_VERSION)\n",
    "\n",
    "def encode(paper):\n",
    "    \"\"\"Tokenize, encode, and pad sentences for BERT input.\"\"\"\n",
    "    return tokenizer(paper[\"sentence\"], truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"source\", \"sentence\", \"input_ids\", \"attention_mask\"])\n",
    "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load and run classifier\n",
    "model = psie.classifier.BertClassifier()\n",
    "state_dict = torch.load(CLASSIFIER_PATH, map_location=device)\n",
    "state_dict.pop(\"bert.embeddings.position_ids\", None)  # Remove unnecessary key\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "pred = model.predict(dataset_loader, device)\n",
    "\n",
    "# Extract predictions (class with highest probability)\n",
    "predictions = []\n",
    "for i in range(len(pred)):\n",
    "    predictions.append(np.argmax(pred[i].cpu().numpy()))\n",
    "\n",
    "# Filter relevant sentences (prediction == 1)\n",
    "filtered_sentences = {\"sentence\": [], \"source\": []}\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == 1:\n",
    "        filtered_sentences[\"sentence\"].append((dataset[i][\"sentence\"]))\n",
    "        filtered_sentences[\"source\"].append((dataset[i][\"source\"]))\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON_1), exist_ok=True)\n",
    "with open(OUTPUT_JSON_1, \"w\") as f:\n",
    "    json.dump(filtered_sentences, f)\n",
    "\n",
    "classifier_time = time.time() - start_time_classifier\n",
    "\n",
    "# ========== Stage 2/3: Named Entity Recognition (NER) ==========\n",
    "start_time_ner = time.time()\n",
    "\n",
    "# Entity label mapping\n",
    "id_to_BOI = {\n",
    "    1: \"B-CHEM\",     # Chemical entity\n",
    "    0: \"O\",          # No entity\n",
    "    2: \"B-BANDGAP\"   # Bandgap value\n",
    "}\n",
    "\n",
    "with open(OUTPUT_JSON_1, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(NER_PATH)\n",
    "\n",
    "sentences = psie.NerUnlabeledDataset(data[\"sentence\"], tokenizer, max_len=MAX_LEN)\n",
    "sources = data[\"source\"]\n",
    "sentences_params = {\n",
    "    'batch_size': 10,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "}\n",
    "sentences_loader = DataLoader(sentences, **sentences_params)\n",
    "\n",
    "model = psie.BertForNer.from_pretrained(NER_PATH, num_labels=3)\n",
    "model.to(device)\n",
    "\n",
    "# NER predictions\n",
    "predictions = model.predict(sentences_loader, device, id_to_BOI)\n",
    "\n",
    "# Extract entity labels from predictions\n",
    "extr_labels = []\n",
    "for n in range(len(predictions)):\n",
    "    tokens = tokenizer.tokenize(\n",
    "        \"[CLS]\" + psie.preprocess_text(sentences[n][\"plain\"]) + \"[SEP]\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    extracted = {}\n",
    "    i = 0\n",
    "    while i < MAX_LEN:\n",
    "        if predictions[n][i] != \"O\" and tokens[i] not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            entity = predictions[n][i]\n",
    "            entry = []\n",
    "            while predictions[n][i] == entity:\n",
    "                entry.append(tokens[i])\n",
    "                i += 1\n",
    "                if i >= MAX_LEN:\n",
    "                    break\n",
    "            if entity in extracted.keys():\n",
    "                extracted[entity].append(\" \".join(entry))\n",
    "            else:\n",
    "                extracted[entity] = [\" \".join(entry)]\n",
    "        i += 1\n",
    "    extr_labels.append(extracted)\n",
    "\n",
    "# Extract sentences with multiple mentions (for relation classification)\n",
    "relational = []\n",
    "for i in range(len(extr_labels)):\n",
    "    n_entries = [len(extr_labels[i][key]) for key in extr_labels[i].keys()]\n",
    "    if n_entries != []:\n",
    "        if len(n_entries) == 2:\n",
    "            if n_entries[0] > 1 and n_entries[1] > 1:\n",
    "                relational.append(extr_labels[i].copy())\n",
    "                relational[-1][\"sentence\"] = sentences[i][\"plain\"]\n",
    "                relational[-1][\"source\"] = sources[i]\n",
    "\n",
    "print(\"Relational/Total: \", len(relational), \"/\", len(predictions))\n",
    "with open(OUTPUT_JSON_2_M, \"w\") as f:\n",
    "    json.dump(relational, f)\n",
    "\n",
    "# Extract sentences with single mentions (exactly 1 material and 1 bandgap)\n",
    "relevant = []\n",
    "for i in range(len(extr_labels)):\n",
    "    n_entries = [len(extr_labels[i][key]) for key in extr_labels[i].keys()]\n",
    "    if n_entries == [1, 1]:\n",
    "        relevant.append(extr_labels[i])\n",
    "        relevant[-1][\"sentence\"] = sentences[i][\"plain\"]\n",
    "        relevant[-1][\"source\"] = sources[i]\n",
    "\n",
    "print(\"Relevant/Total: \", len(relevant), \"/\", len(predictions))\n",
    "\n",
    "# Clean and standardize single-mention extractions\n",
    "database = {\"compound\": [], \"Gap\": [], \"sentence\": [], \"source\": []}\n",
    "\n",
    "for n in range(len(relevant)):\n",
    "    chem, trgt = None, None\n",
    "\n",
    "    try:\n",
    "        # Clean chemical entity name\n",
    "        chem = (\n",
    "            relevant[n][\"B-CHEM\"][0]\n",
    "            .strip()\n",
    "            .replace(\" \", \"\")\n",
    "            .replace(\"#\", \"\")\n",
    "            .replace(\"(\", \"\\(\")\n",
    "            .replace(\")\", \"\\)\")\n",
    "            .replace(\"+\", \"\\+\")\n",
    "            .replace(\"[UNK]\", \"\")\n",
    "            .replace(\".\", \"\\.\")\n",
    "        )\n",
    "\n",
    "        chem = re.findall(\n",
    "            \"(?i)[^a-zA-Z0-9]*\" + chem + \"[^a-zA-Z]\",\n",
    "            relevant[n][\"sentence\"],\n",
    "        )[0].strip()\n",
    "\n",
    "        if chem.endswith(\",\") or chem.endswith(\".\"):\n",
    "            chem = chem[0 : len(chem) - 1]\n",
    "        if chem.startswith(\",\") or chem.startswith(\".\"):\n",
    "            chem = chem[1 : len(chem)]\n",
    "\n",
    "        # Convert element names to symbols\n",
    "        if chem in psie.ELEMENT_NAMES:\n",
    "            chem = psie.ELEMENTS[psie.ELEMENT_NAMES.index(chem)]\n",
    "\n",
    "        # Clean bandgap value\n",
    "        trgt = relevant[n][id_to_BOI[2]][0].replace(\"#\", \"\").strip()\n",
    "        trgt = (\n",
    "            trgt.replace(\"[\", \"\")\n",
    "            .replace(\"]\", \"\")\n",
    "            .replace(\"{\", \"\")\n",
    "            .replace(\"}\", \"\")\n",
    "            .replace(\"=\", \"\")\n",
    "            .replace(\"[UNK]\", \"\")\n",
    "        )\n",
    "\n",
    "        trgt = trgt.replace(\"ev\", \"eV\")\n",
    "\n",
    "        if trgt.endswith(\",\") or trgt.endswith(\".\"):\n",
    "            trgt = trgt[0 : len(trgt) - 1]\n",
    "        if trgt.startswith(\",\") or trgt.startswith(\".\"):\n",
    "            trgt = trgt[1 : len(trgt)]\n",
    "\n",
    "        if (chem is not None) and (trgt is not None):\n",
    "            database[\"compound\"].append(chem)\n",
    "            database[\"Gap\"].append(trgt)\n",
    "\n",
    "        database[\"sentence\"].append(relevant[n][\"sentence\"])\n",
    "        database[\"source\"].append(relevant[n][\"source\"])\n",
    "\n",
    "    except:\n",
    "        comp = (\n",
    "            relevant[n][\"B-CHEM\"][0]\n",
    "            .replace(\"#\", \"\")\n",
    "            .replace(\" \", \"\")\n",
    "            .replace(\"(\", \"\\(\")\n",
    "            .replace(\")\", \"\\)\")\n",
    "            .replace(\"+\", \"\\+\")\n",
    "            .replace(\"[UNK]\", \"\")\n",
    "        )\n",
    "        trgt = relevant[n][id_to_BOI[2]][0].replace(\"#\", \"\").strip()\n",
    "        print(comp, trgt, relevant[n][\"sentence\"], \"\\n\\n\")\n",
    "\n",
    "print(\"Database entries:\", len(database[\"compound\"]), \"/\", len(relevant))\n",
    "\n",
    "# Validate chemical formulas using pymatgen\n",
    "database = pd.DataFrame(database)\n",
    "valid_i = []\n",
    "\n",
    "for i, mat in enumerate(database[\"compound\"]):\n",
    "    try:\n",
    "        Composition(mat).get_reduced_formula_and_factor()[0]\n",
    "        valid_i.append(i)\n",
    "    except:\n",
    "        print(mat, \"\\t\", database[\"sentence\"][i], \"\\n\\n\")\n",
    "\n",
    "print(\"Database entries:\", len(valid_i), \"/\", len(relevant))\n",
    "database.iloc[valid_i].to_csv(OUTPUT_CSV_2_S)\n",
    "\n",
    "ner_time = time.time() - start_time_ner\n",
    "\n",
    "# ========== Stage 3/3: Relation Classification ==========\n",
    "start_time_relation = time.time()\n",
    "\n",
    "# Add relation extraction tokens to BERT vocabulary\n",
    "tokenizer = BertTokenizerFast.from_pretrained(RELATION_PATH)\n",
    "new_tokens = [\"[E1]\", \"[/E1]\", \"[E2]\", \"[/E2]\"]\n",
    "tokenizer.add_tokens(list(new_tokens))\n",
    "\n",
    "# Load multi-mention data\n",
    "with open(OUTPUT_JSON_2_M, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(data[0])\n",
    "\n",
    "# Add entity tags around materials and bandgap values\n",
    "data = psie.fromNer(data)\n",
    "\n",
    "print(data[\"sentence\"][0])\n",
    "print(data[\"sentence\"][1])\n",
    "\n",
    "# Filter sentences containing both entity tags\n",
    "ner_dataset = {\"sentence\": [], \"isrelated\": [], \"source\": []}\n",
    "for i in range(len(data[\"sentence\"])):\n",
    "    if (\"[E1]\" in data[\"sentence\"][i]) and (\"[E2]\" in data[\"sentence\"][i]):\n",
    "        ner_dataset[\"sentence\"].append(str(data[\"sentence\"][i]))\n",
    "        ner_dataset[\"isrelated\"].append(None)\n",
    "        ner_dataset[\"source\"].append(data[\"source\"][i])\n",
    "print(len(ner_dataset[\"sentence\"]), \"/\", len(data[\"sentence\"]))\n",
    "\n",
    "ner = psie.RelationDataset(ner_dataset, tokenizer, max_len=MAX_LEN)\n",
    "ner_params = {\"batch_size\": 8, \"shuffle\": False, \"num_workers\": 0}\n",
    "ner_loader = DataLoader(ner, **ner_params)\n",
    "\n",
    "model = psie.BertForRelations(\n",
    "    pretrained=RELATION_PATH, dropout=0.2, use_cls_embedding=True\n",
    ")\n",
    "model.bert.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# Predict relations\n",
    "pred = model.predict(ner_loader, device)\n",
    "predictions = []\n",
    "for i in range(len(pred)):\n",
    "    predictions.append(np.argmax(pred[i].cpu().numpy()))\n",
    "\n",
    "# Extract confirmed relations\n",
    "database = {\"compound\": [], \"Gap\": [], \"sentence\": [], \"source\": []}\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == 1:\n",
    "        # Extract material (E1) and bandgap (E2) from tagged sentence\n",
    "        comp = re.findall(\n",
    "            re.escape(\"[E1]\") + \".*\" + re.escape(\"[/E1]\"), ner_dataset[\"sentence\"][i]\n",
    "        )\n",
    "        temp = re.findall(\n",
    "            re.escape(\"[E2]\") + \".*\" + re.escape(\"[/E2]\"), ner_dataset[\"sentence\"][i]\n",
    "        )\n",
    "\n",
    "        if (len(comp) > 0) and (len(temp) > 0):\n",
    "            comp = comp[0].replace(\"[E1]\", \"\").replace(\"[/E1]\", \"\").replace(\" \", \"\")\n",
    "            temp = temp[0].replace(\"[E2]\", \"\").replace(\"[/E2]\", \"\").replace(\" \", \"\")\n",
    "            database[\"compound\"].append(comp)\n",
    "            database[\"Gap\"].append(temp)\n",
    "            database[\"sentence\"].append(ner_dataset[\"sentence\"][i])\n",
    "            database[\"source\"].append(ner_dataset[\"source\"][i])\n",
    "\n",
    "# Validate chemical formulas\n",
    "database = pd.DataFrame(database)\n",
    "valid_i = []\n",
    "for i, comp in enumerate(database[\"compound\"]):\n",
    "    try:\n",
    "        Composition(comp).get_reduced_formula_and_factor()[0]\n",
    "        valid_i.append(i)\n",
    "    except:\n",
    "        print(comp, \"\\t\", database[\"sentence\"][i], \"\\n\\n\")\n",
    "print(\"Database entries:\", len(valid_i), \"/\", len(database[\"sentence\"]))\n",
    "\n",
    "database.iloc[valid_i].to_csv(OUTPUT_CSV_3)\n",
    "\n",
    "relation_time = time.time() - start_time_relation\n",
    "total_time = time.time() - start_time_total\n",
    "\n",
    "# Save runtime statistics\n",
    "with open(RUNTIME, 'w') as f:\n",
    "    f.write(f\"Classifier time: {classifier_time:.2f} seconds\\n\")\n",
    "    f.write(f\"NER time: {ner_time:.2f} seconds\\n\")\n",
    "    f.write(f\"Relation time: {relation_time:.2f} seconds\\n\")\n",
    "    f.write(f\"Total time: {total_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ChatExtract (CE)\n",
    "\n",
    "**Kernel:** lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "import logging\n",
    "from datetime import datetime\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "import time\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "def inference(MODEL, CODE, PROPERTY, TXT_DIR, OUTPUT_DIR, logger):\n",
    "    \"\"\"Run two-stage extraction: (1) filter relevant sentences, (2) extract data.\n",
    "    \n",
    "    Args:\n",
    "        MODEL: LLM model name\n",
    "        CODE: Extraction method code identifier\n",
    "        PROPERTY: Property to extract (e.g., \"band gap\")\n",
    "        TXT_DIR: Directory containing text files\n",
    "        OUTPUT_DIR: Output directory for results\n",
    "        logger: Logger instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========== Stage 1: Filter relevant sentences ==========\n",
    "    def get_positive_sentences(model_name):\n",
    "        \"\"\"Identify sentences related to the target property using LLM classification.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the LLM model\n",
    "            \n",
    "        Returns:\n",
    "            Path to CSV file containing positive sentences\n",
    "        \"\"\"\n",
    "        MODEL_NAME = model_name.replace(\":\", \"-\")\n",
    "        POSITIVE_CSV = os.path.join(OUTPUT_DIR, f\"1_positive_sentences_{MODEL_NAME}_{CODE}.csv\")\n",
    "\n",
    "        classif_q = f'Is the following sentence related to \"{PROPERTY}\"? Answer only \"Yes\" or \"No\" without any explanation:'\n",
    "\n",
    "        # Load existing results or initialize new DataFrame\n",
    "        try:\n",
    "            df_positive = pd.read_csv(POSITIVE_CSV)\n",
    "            processed_dois = set(df_positive[\"doi\"].unique())\n",
    "        except FileNotFoundError:\n",
    "            df_positive = pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"original_index\",\n",
    "                    \"positive_sentences\",\n",
    "                    \"integrated_sentences\",\n",
    "                    \"doi\",\n",
    "                ]\n",
    "            )\n",
    "            processed_dois = set()\n",
    "\n",
    "        # Initialize LLM\n",
    "        llm = ChatOllama(model=model_name, temperature=0)\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"You are an expert extraction algorithm specialized in materials science.\",\n",
    "                ),\n",
    "                (\"human\", \"{question}\\n{text}\\n\"),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | llm\n",
    "\n",
    "        # Process text files\n",
    "        txt_files = glob.glob(os.path.join(TXT_DIR, \"*.txt\"))\n",
    "        logger.info(\n",
    "            f\"Found {len(txt_files)} text files for processing with {model_name}\"\n",
    "        )\n",
    "\n",
    "        for txt_path in tqdm(txt_files, desc=f\"Processing {model_name}\"):\n",
    "            doi = os.path.basename(txt_path).replace(\".txt\", \"\").replace(\"_\", \"/\")\n",
    "            if doi in processed_dois:\n",
    "                continue\n",
    "\n",
    "            # Read sentences from file\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                sentences = [line.strip() for line in f]\n",
    "\n",
    "            # Classify each sentence\n",
    "            results = []\n",
    "            for idx, sentence in enumerate(sentences):\n",
    "                try:\n",
    "                    answer = chain.invoke(\n",
    "                        {\"question\": classif_q, \"text\": sentence}\n",
    "                    ).content\n",
    "                    answer = re.sub(r\"[^\\w\\s]\", \"\", answer).strip().lower()\n",
    "                    results.append((idx, sentence, 1 if answer == \"yes\" else 0))\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing sentence {idx} in {doi}: {str(e)}\")\n",
    "                    results.append((idx, sentence, 0))\n",
    "\n",
    "            # Generate integrated sentences (previous sentence + current sentence)\n",
    "            positive_data = []\n",
    "            for idx, (sentence_idx, sentence, label) in enumerate(results):\n",
    "                if label == 1:\n",
    "                    integrated = (\n",
    "                        f\"{results[idx-1][1]} {sentence}\" if idx > 0 else sentence\n",
    "                    )\n",
    "                    positive_data.append(\n",
    "                        {\n",
    "                            \"original_index\": f\"{sentence_idx}/{len(sentences)}\",\n",
    "                            \"positive_sentences\": sentence,\n",
    "                            \"integrated_sentences\": integrated,\n",
    "                            \"doi\": doi,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # Save results periodically\n",
    "            if positive_data:\n",
    "                df_positive = pd.concat([df_positive, pd.DataFrame(positive_data)])\n",
    "                df_positive.to_csv(POSITIVE_CSV, index=False, escapechar='\\\\')\n",
    "                logger.info(f\"Saved {len(positive_data)} positive sentences from {doi}\")\n",
    "\n",
    "        return POSITIVE_CSV\n",
    "\n",
    "    # ========== Stage 2: Extract structured data ==========\n",
    "    def extract_data(model_name, csv_path):\n",
    "        \"\"\"Extract structured bandgap data from positive sentences.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the LLM model\n",
    "            csv_path: Path to CSV containing positive sentences\n",
    "        \"\"\"\n",
    "        TEMPERATURE = 0\n",
    "        CONTEXT = 4096  # Default: 2048\n",
    "        MODEL_NAME = model_name.replace(\":\", \"-\")\n",
    "\n",
    "        # Initialize output files\n",
    "        EXTRACTED_CSV = os.path.join(\n",
    "            OUTPUT_DIR, f\"2_extracted_{MODEL_NAME}_{CODE}.csv\"\n",
    "        )\n",
    "        BINCLAS_CSV = os.path.join(\n",
    "            OUTPUT_DIR, f\"2_binclas_{MODEL_NAME}_{CODE}.csv\"\n",
    "        )\n",
    "        DIALOGUE_CSV = os.path.join(\n",
    "            OUTPUT_DIR, f\"2_dialogues_{MODEL_NAME}_{CODE}.csv\"\n",
    "        )\n",
    "\n",
    "        # Create empty CSV with headers\n",
    "        pd.DataFrame(\n",
    "            columns=[\n",
    "                \"passage\",\n",
    "                \"sentence\",\n",
    "                \"doi\",\n",
    "                \"material\",\n",
    "                \"value\",\n",
    "                \"unit\",\n",
    "                \"material_valid\",\n",
    "                \"value_valid\",\n",
    "                \"unit_valid\",\n",
    "            ]\n",
    "        ).to_csv(EXTRACTED_CSV, index=False)\n",
    "\n",
    "        # Define prompts\n",
    "        classif_q = f'Answer only \"Yes\" or \"No\" without any explanation. Based on the following text, is there a value of **{PROPERTY}** mentioned in it?\\n\\n'\n",
    "        ifmulti_q = f'Answer \"Yes\" or \"No\" only. Does the following text mention more than one value of **{PROPERTY}**?\\n\\n'\n",
    "        single_q = [\n",
    "            f'Give the number only without units, do not use a full sentence. If the value is not present in the text, type \"None\". What is the value of the **{PROPERTY}** in the following text?\\n\\n',\n",
    "            f'Give the unit only, do not use a full sentence. If the unit is not present in the text, type \"None\". What is the unit of the **{PROPERTY}** in the following text?\\n\\n',\n",
    "            f'Give the name of the material only, do not use a full sentence. If the name of the material is not present in the text, type \"None\". What is the material for which the **{PROPERTY}** is given in the following text?\\n\\n',\n",
    "        ]\n",
    "        singlefollowup_q = [\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                f\" the value of the **{PROPERTY}** for the material in the following text?\\n\\n\",\n",
    "            ],\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                f\" the unit of the value of **{PROPERTY}** in the following text?\\n\\n\",\n",
    "            ],\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                f\" the material for which the value of **{PROPERTY}** is given in the following text? Make sure it is a real material.\\n\\n\",\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        tab_q = f'Use only data present in the text. If data is not present in the text, type \"None\". Summarize the values of **{PROPERTY}** in the following text in a form of a table consisting of: Material, Value, Unit. Ensure that the \"Value\" and \"Unit\" are separated into different columns.\\n\\n'\n",
    "        tabfollowup_q = [\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                \" the \",\n",
    "                f\" material for which the value of **{PROPERTY}** is given in the following text? Make sure it is a real material.\\n\\n\",\n",
    "            ],\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                f\" the value of the **{PROPERTY}** for the \",\n",
    "                \" material in the following text?\\n\\n\",\n",
    "            ],\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                \" the unit of the \",\n",
    "                f\" value of **{PROPERTY}** in the following text?\\n\\n\",\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        it = [\n",
    "            \"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\",\n",
    "            \"eighth\", \"ninth\", \"tenth\", \"eleventh\", \"twelfth\", \"thirteenth\",\n",
    "            \"fourteenth\", \"fifteenth\", \"sixteenth\", \"seventeenth\", \"eighteenth\",\n",
    "            \"nineteenth\", \"twentieth\",\n",
    "        ]\n",
    "        col = [\"Material\", \"Value\", \"Unit\"]\n",
    "        single_cols = [\"value\", \"unit\", \"material\"]\n",
    "\n",
    "        # Initialize LLM\n",
    "        llm = ChatOllama(model=model_name, temperature=TEMPERATURE, num_ctx=CONTEXT)\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"You are an expert extraction algorithm specialized in materials science.\",\n",
    "                ),\n",
    "                (\"placeholder\", \"{conversation}\"),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | llm\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            ntot = len(df)\n",
    "            logger.info(f\"Starting data extraction for {len(df)} entries\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read CSV: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        with tqdm(total=ntot, desc=f\"Extracting {MODEL_NAME}\") as pbar:\n",
    "            for i in range(ntot):\n",
    "                try:\n",
    "                    binary_classif = []\n",
    "                    sss = []\n",
    "                    sss.append((\"human\", classif_q + df[\"positive_sentences\"][i]))\n",
    "                    ans = chain.invoke({\"conversation\": sss}).content\n",
    "                    sss.append((\"ai\", ans))\n",
    "                    \n",
    "                    if \"yes\" in ans.strip().lower():  # Positive classification\n",
    "                        binary_classif.append(1)\n",
    "                        result = {}\n",
    "                        passage = df[\"integrated_sentences\"][i]\n",
    "                        sentence = df[\"positive_sentences\"][i]\n",
    "                        sss.append((\"human\", ifmulti_q + passage))\n",
    "                        ans = chain.invoke({\"conversation\": sss}).content\n",
    "                        sss.append((\"ai\", ans))\n",
    "                        \n",
    "                        if \"no\" in ans.lower():  # Single data point\n",
    "                            result[\"passage\"] = [passage]\n",
    "                            result[\"sentence\"] = [sentence]\n",
    "                            result[\"doi\"] = [df[\"doi\"][i]]\n",
    "                            result[\"material\"] = []\n",
    "                            result[\"value\"] = []\n",
    "                            result[\"unit\"] = []\n",
    "                            result[\"material_valid\"] = []\n",
    "                            result[\"value_valid\"] = []\n",
    "                            result[\"unit_valid\"] = []\n",
    "                            \n",
    "                            for j in range(len(single_q)):\n",
    "                                sss.append((\"human\", single_q[j] + passage))\n",
    "                                ans = chain.invoke({\"conversation\": sss}).content\n",
    "                                sss.append((\"ai\", ans))\n",
    "                                result[single_cols[j]].append(ans)\n",
    "                                if \"none\" in ans.lower():\n",
    "                                    result[single_cols[j] + \"_valid\"].append(0)\n",
    "                                else:\n",
    "                                    result[single_cols[j] + \"_valid\"].append(1)\n",
    "                                    \n",
    "                        elif \"yes\" in ans.lower():  # Multiple data points\n",
    "                            sss.append((\"human\", tab_q + passage))\n",
    "                            tab = chain.invoke({\"conversation\": sss}).content\n",
    "                            sss.append((\"ai\", tab))\n",
    "                            sst = copy(sss)\n",
    "\n",
    "                            # Parse markdown table\n",
    "                            start_index = tab.find(\"|\")\n",
    "                            end_index = tab.rfind(\"|\") + 1\n",
    "                            cleaned_output = tab[start_index:end_index].strip()\n",
    "                            lines = cleaned_output.split(\"\\n\")\n",
    "                            \n",
    "                            # Parse data rows\n",
    "                            data = []\n",
    "                            for line in lines[2:]:\n",
    "                                stripped_line = line.strip(\"| \")\n",
    "                                parts = [p.strip() for p in stripped_line.split(\"|\")]\n",
    "                                \n",
    "                                if len(parts) == 3:\n",
    "                                    data.append(parts)\n",
    "                                elif len(parts) < 3:\n",
    "                                    while len(parts) < 3:\n",
    "                                        parts.append(None)\n",
    "                                    data.append(parts)\n",
    "                                elif len(parts) > 3:\n",
    "                                    material = parts[0].strip()\n",
    "                                    value = parts[1].strip()\n",
    "                                    sentence = \"|\".join(parts[2:]).strip()\n",
    "                                    data.append([material, value, sentence])\n",
    "                                else:\n",
    "                                    continue\n",
    "\n",
    "                            tab = pd.DataFrame(data, columns=col)\n",
    "\n",
    "                            result[\"passage\"] = []\n",
    "                            result[\"sentence\"] = []\n",
    "                            result[\"doi\"] = []\n",
    "                            result[\"material\"] = []\n",
    "                            result[\"value\"] = []\n",
    "                            result[\"unit\"] = []\n",
    "                            result[\"material_valid\"] = []\n",
    "                            result[\"value_valid\"] = []\n",
    "                            result[\"unit_valid\"] = []\n",
    "\n",
    "                            for k in range(len(tab)):\n",
    "                                sst.append(\n",
    "                                    (\n",
    "                                        \"tab\",\n",
    "                                        f\"{tab[col[0]][k]},{tab[col[1]][k]},{tab[col[2]][k]}\",\n",
    "                                    )\n",
    "                                )\n",
    "                                result[\"passage\"].append(passage)\n",
    "                                result[\"sentence\"].append(sentence)\n",
    "                                result[\"doi\"].append(df[\"doi\"][i])\n",
    "                                multi_valid = True\n",
    "                                \n",
    "                                for l in range(3):\n",
    "                                    temp_r = str(tab[col[l]][k])\n",
    "                                    ss = (\n",
    "                                        tabfollowup_q[l][0]\n",
    "                                        + temp_r\n",
    "                                        + tabfollowup_q[l][1]\n",
    "                                        + it[k]\n",
    "                                        + tabfollowup_q[l][2]\n",
    "                                        + passage\n",
    "                                    )\n",
    "                                    result[col[l].lower()].append(temp_r)\n",
    "                                    \n",
    "                                    if \"none\" in temp_r.lower():\n",
    "                                        result[col[l].lower() + \"_valid\"].append(0)\n",
    "                                        multi_valid = False\n",
    "                                    elif multi_valid:\n",
    "                                        sss.append((\"human\", ss))\n",
    "                                        sst.append((\"human\", ss))\n",
    "                                        ans = chain.invoke({\"conversation\": sss}).content\n",
    "                                        sss.append((\"ai\", ans))\n",
    "                                        sst.append((\"ai\", ans))\n",
    "                                        \n",
    "                                        if \"no\" in ans.lower():\n",
    "                                            result[col[l].lower() + \"_valid\"].append(0)\n",
    "                                            multi_valid = False\n",
    "                                        else:\n",
    "                                            result[col[l].lower() + \"_valid\"].append(1)\n",
    "                                    else:\n",
    "                                        result[col[l].lower() + \"_valid\"].append(1)\n",
    "                        \n",
    "                        try:\n",
    "                            pd.DataFrame(result).to_csv(\n",
    "                                EXTRACTED_CSV, mode=\"a\", index=False, header=False\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(\"Appending extracted data error: \", i, \"  \", e)\n",
    "                            print(\"Appending extracted data error: \", result, \"  \", e)\n",
    "                            print(\"Appending extracted data error: \", tab, \"  \", e)\n",
    "                    else:  # Negative classification\n",
    "                        binary_classif.append(0)\n",
    "                    \n",
    "                    pd.DataFrame(binary_classif).to_csv(\n",
    "                        BINCLAS_CSV, mode=\"a\", index=False, header=False\n",
    "                    )\n",
    "                    \n",
    "                    try:\n",
    "                        pd.DataFrame(sst).to_csv(\n",
    "                            DIALOGUE_CSV, mode=\"a\", index=False, header=False\n",
    "                        )\n",
    "                        del sst\n",
    "                    except:\n",
    "                        pd.DataFrame(sss).to_csv(\n",
    "                            DIALOGUE_CSV, mode=\"a\", index=False, header=False\n",
    "                        )\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row {i}: {str(e)}\")\n",
    "                    print(f\"Ignoring {i+1}/{ntot} ({round(i/ntot*100,1)} %)\")\n",
    "                    continue\n",
    "\n",
    "        logger.info(f\"Completed data extraction for {model_name}\")\n",
    "        return EXTRACTED_CSV\n",
    "\n",
    "    # Execute pipeline\n",
    "    stage1_output = get_positive_sentences(MODEL)\n",
    "    EXTRACTED_CSV = extract_data(MODEL, stage1_output)\n",
    "    return EXTRACTED_CSV\n",
    "\n",
    "\n",
    "def process_chat(MODELS, PROPERTY, TXT_DIR, OUTPUT_DIR):\n",
    "    \"\"\"Process multiple LLM models for bandgap extraction.\n",
    "    \n",
    "    Args:\n",
    "        MODELS: Dictionary mapping model names to codes\n",
    "        PROPERTY: Property to extract\n",
    "        TXT_DIR: Input text directory\n",
    "        OUTPUT_DIR: Output directory\n",
    "    \"\"\"\n",
    "    # Initialize logging\n",
    "    start_time = time.time()\n",
    "    log_file = os.path.join(OUTPUT_DIR, f\"processing_{dtime}.log\")\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    logger.info(\"Initializing inference pipeline...\")\n",
    "    logger.info(f\"Property: {PROPERTY}\")\n",
    "    logger.info(f\"Input TXT directory: {TXT_DIR}\")\n",
    "    logger.info(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    for MODEL, CODE in MODELS.items():\n",
    "        logger.info(f\"\\n{'='*40}\")\n",
    "        logger.info(f\"Processing model: {MODEL}\")\n",
    "        inference(MODEL, CODE, PROPERTY, TXT_DIR, OUTPUT_DIR, logger)\n",
    "        logger.info(f\"Completed processing for {MODEL}\")\n",
    "\n",
    "    logger.info(f\"\\n{'='*40}\")\n",
    "    logger.info(f\"Total processing time: {time.time()-start_time:.2f} seconds\")\n",
    "    logger.info(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    logger.info(\"Processing complete!\")\n",
    "\n",
    "\n",
    "# Execute extraction\n",
    "TXT_DIR = os.path.join(HOME, \"TXT(fromPDF_processed)\")\n",
    "OUTPUT_DIR = os.path.join(HOME, \"output\", \"3-ChatExtract\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MODELS = {\n",
    "    \"llama2:13b\": \"CE_1\",\n",
    "    \"llama3.1:70b\": \"CE_2\",\n",
    "    \"qwen2.5:14b\": \"CE_3\",\n",
    "}\n",
    "\n",
    "process_chat(\n",
    "    MODELS=MODELS,\n",
    "    PROPERTY=\"band gap\",\n",
    "    TXT_DIR=TXT_DIR,\n",
    "    OUTPUT_DIR=OUTPUT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 LangChain RAG (LC)\n",
    "\n",
    "**Kernel:** lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0\n",
    "TOP_K = 5\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "\n",
    "def process_single_pdf(pdf_file, embeddings, llm, text_splitter, prompt, question, client):\n",
    "    \"\"\"Process a single PDF file using RAG (Retrieval-Augmented Generation).\n",
    "    \n",
    "    This function:\n",
    "    1. Loads and chunks the PDF document\n",
    "    2. Creates a vector database for semantic search\n",
    "    3. Retrieves relevant chunks based on the question\n",
    "    4. Generates structured output using LLM\n",
    "    \n",
    "    Args:\n",
    "        pdf_file: Path to PDF file\n",
    "        embeddings: Embedding model for vectorization\n",
    "        llm: Language model for generation\n",
    "        text_splitter: Text chunking strategy\n",
    "        prompt: Prompt template\n",
    "        question: Query question\n",
    "        client: ChromaDB client instance\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (doi, output, status) where status indicates if output is a valid table\n",
    "    \"\"\"\n",
    "    doi = os.path.basename(pdf_file).replace('.pdf', '').replace('_', '/')\n",
    "    try:\n",
    "        collection_name = f\"collection_{doi.replace('/', '_')}\"\n",
    "        \n",
    "        # Force delete old collection if exists\n",
    "        try:\n",
    "            client.delete_collection(collection_name)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Load and split document\n",
    "        loader = PyMuPDFLoader(file_path=pdf_file)\n",
    "        docs = loader.load()\n",
    "        chunks = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Create vector database\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            collection_name=collection_name,\n",
    "            client=client\n",
    "        )\n",
    "        \n",
    "        # Build and execute RAG chain\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        output = rag_chain.invoke(question).content\n",
    "        status = 1 if output.strip().startswith(\"|\") else 0\n",
    "\n",
    "        # Clean up resources\n",
    "        del loader, docs, chunks, vector_db, retriever, rag_chain\n",
    "        gc.collect()\n",
    "        \n",
    "        return doi, output, status\n",
    "\n",
    "    except Exception as e:\n",
    "        return doi, f\"Processing error: {str(e)}\", -1\n",
    "\n",
    "def process_pdfs_batch(codes, embed_stream, infer_stream, output_dir, pdf_dir, template, question):\n",
    "    \"\"\"Batch process PDF files using multiple embedding/LLM combinations.\n",
    "    \n",
    "    Args:\n",
    "        codes: Processing code identifiers\n",
    "        embed_stream: Embedding model stream (paired with inference models)\n",
    "        infer_stream: Inference model stream (paired with embedding models)\n",
    "        output_dir: Output directory\n",
    "        pdf_dir: PDF files directory\n",
    "        template: Prompt template\n",
    "        question: Query question\n",
    "    \"\"\"\n",
    "    # Create global ChromaDB client (in-memory to avoid file residue)\n",
    "    client_settings = Settings(persist_directory=\"\")\n",
    "    global_client = Client(client_settings)\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for code, embed_model, infer_model in zip(codes, embed_stream, infer_stream):\n",
    "        output_csv = os.path.join(output_dir, f\"output_{code}_{dtime}.csv\")\n",
    "        \n",
    "        # Initialize models and tools\n",
    "        embeddings = OllamaEmbeddings(model=embed_model)\n",
    "        llm = ChatOllama(model=infer_model, temperature=TEMPERATURE, num_predict=80)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE, \n",
    "            chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "        # Process each PDF file\n",
    "        pdf_files = sorted(glob.glob(os.path.join(pdf_dir, \"*.pdf\")))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for pdf_file in tqdm(pdf_files, desc=f\"Processing PDF files - {code}\"):\n",
    "            doi, output, status = process_single_pdf(\n",
    "                pdf_file, embeddings, llm, text_splitter, prompt, question, global_client\n",
    "            )\n",
    "            \n",
    "            # Save result\n",
    "            df_new = pd.DataFrame([{'doi': doi, 'output': output, 'status': status}])\n",
    "            if os.path.exists(output_csv):\n",
    "                df_new.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                df_new.to_csv(output_csv, index=False)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "        \n",
    "        # Write to log file\n",
    "        with open(log_file, 'a') as lf:\n",
    "            log_entry = (\n",
    "                f\"Code: {code} | Embed Model: {embed_model} | Infer Model: {infer_model}\\n\"\n",
    "                f\"Start: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))} | \"\n",
    "                f\"End: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))} | \"\n",
    "                f\"Duration: {batch_time:.2f} seconds | Status: {'Success' if status == 1 else 'Error'}\\n\"\n",
    "                f\"{'='*40}\\n\"\n",
    "            )\n",
    "            lf.write(log_entry)\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    total_time = total_end_time - total_start_time\n",
    "    \n",
    "    with open(log_file, 'a') as lf:\n",
    "        log_entry = (\n",
    "            f\"Total Start: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time))}\\n\"\n",
    "            f\"Total End: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_end_time))}\\n\"\n",
    "            f\"Total Duration: {total_time:.2f} seconds\\n\"\n",
    "            f\"{'='*40}\\n\"\n",
    "        )\n",
    "        lf.write(log_entry)\n",
    "\n",
    "# Prompt template for extraction\n",
    "TEMPLATE = \"\"\"\n",
    "You are an expert information extraction algorithm.\n",
    "Extract all the band gap values in the CONTEXT given below.\n",
    "Output the band gap values in the form of a markdown table, including: Material (name of the material), Value (band gap value), Unit (unit of value).\n",
    "Do not explain, only output the table in markdown format.\n",
    "The output is strictly in the following format.\n",
    "| Material | Value | Unit |\n",
    "|----------|-------|------|\n",
    "| ... | ... | eV |\n",
    "| ... | ... | meV |\n",
    "If no band gap values mentioned in the article, the following table is acceptable:\n",
    "| Material | Value | Unit |\n",
    "|----------|-------|------|\n",
    "| None | None | None |\n",
    "---\n",
    "CONTEXT: {context}\n",
    "---\n",
    "QUESTION: {question}\n",
    "Answer in markdown table:\n",
    "\"\"\"\n",
    "QUESTION = \"What are the materials' name and their band gap values?\"\n",
    "\n",
    "TXT_DIR = os.path.join(HOME, \"TXT(fromPDF_processed)\")\n",
    "PDF_DIR = os.path.join(HOME, \"PDF\")\n",
    "OUTPUT_DIR = os.path.join(HOME, \"output\", \"4-LangChain\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "log_file = os.path.join(OUTPUT_DIR, f\"log_{dtime}.log\")\n",
    "\n",
    "EMBEDDING_MODELS = [\n",
    "    \"nomic-embed-text\",\n",
    "    \"bge-m3\",\n",
    "]\n",
    "INFERENCE_MODELS = [\n",
    "    \"llama2:13b\",\n",
    "    \"llama3.1:70b\",\n",
    "    \"qwen2.5:14b\",\n",
    "]\n",
    "\n",
    "# Generate model combination codes\n",
    "CODES = [f\"LC_{i+1}{j+1}\" \n",
    "        for i in range(len(EMBEDDING_MODELS)) \n",
    "        for j in range(len(INFERENCE_MODELS))]\n",
    "embed_stream = [model for model in EMBEDDING_MODELS for _ in INFERENCE_MODELS]\n",
    "infer_stream = INFERENCE_MODELS * len(EMBEDDING_MODELS)\n",
    "\n",
    "# Execute batch processing\n",
    "process_pdfs_batch(\n",
    "    CODES,\n",
    "    embed_stream,\n",
    "    infer_stream,\n",
    "    OUTPUT_DIR,\n",
    "    PDF_DIR,\n",
    "    TEMPLATE,\n",
    "    QUESTION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Kimi-1.5 (Kimi)\n",
    "\n",
    "Use the prompt below to extract bandgap data from Kimi (<https://www.kimi.com>) and save the output to `KIMI_OUT`.\n",
    "\n",
    "### Prompt:\n",
    "\n",
    "```\n",
    "You are an expert information extraction algorithm.\n",
    "Extract all the band gap values in this article and output them in the form of a markdown table, including: Material (name of the material), Value (value with unit), Sentence (the sentence from which this data record comes).\n",
    "If data is not present in the article, type \"None\". \n",
    "Table only, no need for explanation or any other content.\n",
    "The output is strictly in the following format.\n",
    "```markdown\n",
    "| Material | Value | Sentence |\n",
    "|----------|-------|---------|\n",
    "| Material1 | 0.1 eV | ... Eg of Material1 is 0.1 eV ... |\n",
    "| Material1 | 200 meV | Material1 has a band gap of 200 meV, so ... |\n",
    "| Material2 | None | Material2 ... |\n",
    "```\n",
    "\n",
    "If no band gap values mentioned in the article, the following table is acceptable:\n",
    "```markdown\n",
    "| Material | Value | Sentence |\n",
    "|----------|-------|----------|\n",
    "| None | None | None |\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Organize Results\n",
    "\n",
    "Post-processing steps to organize and compare extraction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from my_post import clean_and_normalize, compare_with_index, parse_markdown_table, clean_illegal_chars\n",
    "\n",
    "# Configuration\n",
    "MARK = \"test\"\n",
    "dir_mark = \"\" if MARK == \"\" else f'_{MARK}'\n",
    "manual_xlsx = os.path.join(HOME, \"manual_pub.xlsx\")\n",
    "today = datetime.now().strftime(\"%m%d\")\n",
    "comparison_dir = os.path.join(HOME, f'comparison_{today}{dir_mark}')\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "comparison_xlsx = os.path.join(comparison_dir, \"comparison_pub.xlsx\")\n",
    "\n",
    "# Read manual annotations sheet\n",
    "df_manual = pd.read_excel(manual_xlsx, sheet_name='manual')\n",
    "\n",
    "# Filter rows with non-empty 'material' column\n",
    "df_filtered = df_manual[df_manual['material'].notnull()]\n",
    "\n",
    "# Create summary sheet with selected columns\n",
    "df_summary = df_filtered[['index', 'doi', 'material', 'Manual']].copy()\n",
    "df_summary.rename(columns={'Manual': 'value'}, inplace=True)\n",
    "\n",
    "# Save both sheets to comparison Excel file\n",
    "with pd.ExcelWriter(comparison_xlsx) as writer:\n",
    "    df_manual.to_excel(writer, sheet_name='manual', index=False)\n",
    "    df_summary.to_excel(writer, sheet_name='summary', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Process CDE Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_cde(temp_save, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Convert ChemDataExtractor raw data to standard format.\n",
    "\n",
    "    Args:\n",
    "        temp_save: Path to CDE raw data file\n",
    "        xlsx_path: Output Excel file path: \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    records = joblib.load(temp_save)\n",
    "    \n",
    "    # Extract required fields\n",
    "    columns = [\n",
    "        \"Publisher\",\n",
    "        \"DOI\",\n",
    "        \"Name\",\n",
    "        \"Raw_value\",\n",
    "        \"Raw_unit\",\n",
    "        \"Value\",\n",
    "        \"Unit\",\n",
    "        \"specifier\",\n",
    "        \"Text\",\n",
    "        \"Snowball\",\n",
    "        \"AutoSentenceParser\",\n",
    "    ]\n",
    "    \n",
    "    flat_data = []\n",
    "    for item in records:\n",
    "        bandgap = item[\"BandGap\"]\n",
    "        flat_item = {\n",
    "            \"Publisher\": bandgap[\"publisher\"],\n",
    "            \"DOI\": bandgap[\"doi\"],\n",
    "            \"Name\": bandgap[\"compound\"][\"Compound\"][\"names\"][0],\n",
    "            \"Raw_value\": bandgap[\"raw_value\"],\n",
    "            \"Raw_unit\": bandgap[\"raw_units\"],\n",
    "            \"Value\": bandgap[\"value\"],\n",
    "            \"Unit\": bandgap[\"units\"],\n",
    "            \"specifier\": bandgap[\"specifier\"],\n",
    "            \"Text\": bandgap[\"text\"],\n",
    "            \"Snowball\": bandgap[\"Snowball\"],\n",
    "            \"AutoSentenceParser\": bandgap[\"AutoSentenceParser\"],\n",
    "        }\n",
    "        flat_data.append(flat_item)\n",
    "    \n",
    "    # Save raw data\n",
    "    df = pd.DataFrame(flat_data, columns=columns)\n",
    "\n",
    "    # Clean illegal control characters\n",
    "    df = df.map(clean_illegal_chars)\n",
    "\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"0-raw\", index=False)\n",
    "    \n",
    "    # Save simplified data\n",
    "    simplified_df = pd.DataFrame(\n",
    "        {\n",
    "            \"doi\": df[\"DOI\"],\n",
    "            \"material\": df[\"Name\"],\n",
    "            \"value\": df[\"Raw_value\"],\n",
    "            \"unit\": df[\"Raw_unit\"],\n",
    "        }\n",
    "    )\n",
    "    with pd.ExcelWriter(\n",
    "        xlsx_path, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\"\n",
    "    ) as writer:\n",
    "        simplified_df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(\n",
    "        comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\"\n",
    "    ) as writer:\n",
    "        simplified_df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "\n",
    "def postprocess_cde(temp_save, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Main function for CDE extraction post-processing.\n",
    "    \n",
    "    Args:\n",
    "        temp_save: Path to CDE raw data file\n",
    "        xlsx_path: Result Excel file\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    post_cde(temp_save, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code, sheet_name=\"1-raw\")\n",
    "    compare_with_index(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "code = \"CDE\"\n",
    "\n",
    "# Result data paths\n",
    "output_dir = os.path.join(HOME, \"output\", \"1-ChemDataExtractor\")\n",
    "temp_save = os.path.join(output_dir, \"records_general.joblib\")\n",
    "xlsx_path = os.path.join(comparison_dir, f\"1_{code}_{today}.xlsx\")\n",
    "\n",
    "postprocess_cde(temp_save, xlsx_path, comparison_xlsx, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Process PSIE Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_psie(csv_2s, csv_3, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Convert BERT-PSIE raw data to standard format.\n",
    "    \n",
    "    Args:\n",
    "        csv_2s: Path to single-mention CSV file\n",
    "        csv_3: Path to multiple-mention CSV file\n",
    "        xlsx_path: Output Excel file path: \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    # Read and merge CSV files\n",
    "    df1 = pd.read_csv(csv_2s)\n",
    "    df2 = pd.read_csv(csv_3)\n",
    "    df1.columns = df2.columns\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    df = df.iloc[:, 1:]  # Remove first column\n",
    "    df.columns = ['material', 'value', 'sentence', 'doi']\n",
    "    df = df[['sentence', 'doi', 'material', 'value']]  # Reorder columns\n",
    "    \n",
    "    # Clean illegal control characters\n",
    "    df = df.map(clean_illegal_chars)\n",
    "    \n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_psie(csv_2s, csv_3, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Main function for PSIE extraction post-processing.\n",
    "    \n",
    "    Args:\n",
    "        csv_2s: Path to single-mention CSV file\n",
    "        csv_3: Path to multiple-mention CSV file\n",
    "        xlsx_path: Result Excel file\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    post_psie(csv_2s, csv_3, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code)\n",
    "    compare_with_index(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "code = \"PSIE\"\n",
    "\n",
    "# Result data paths\n",
    "output_dir = os.path.join(HOME, \"output\", \"2-BERT-PSIE\")\n",
    "output_csv_2_s = os.path.join(output_dir, \"2-test_extraction_single_mentions.csv\")\n",
    "output_csv_3 = os.path.join(output_dir, \"3-relations_extraction.csv\")\n",
    "xlsx_path = os.path.join(comparison_dir, f\"2_{code}_{today}.xlsx\")\n",
    "\n",
    "postprocess_psie(output_csv_2_s, output_csv_3, xlsx_path, comparison_xlsx, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Process ChatExtract Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_ce(extracted_csv, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Convert ChatExtract raw data to standard format.\n",
    "    \n",
    "    Args:\n",
    "        extracted_csv: Path to extracted CSV file\n",
    "        xlsx_path: Output Excel file path: \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(extracted_csv)\n",
    "    \n",
    "    # Filter rows with all validity flags set to 1\n",
    "    df = df[(df[\"material_valid\"] == 1) &\n",
    "            (df[\"value_valid\"] == 1) &\n",
    "            (df[\"unit_valid\"] == 1)]\n",
    "    \n",
    "    # Remove \"meV\" or \"eV\" from value column\n",
    "    df[\"value\"] = df[\"value\"].apply(\n",
    "        lambda x: x.replace(\"meV\", \"\").replace(\"eV\", \"\").strip() if pd.notna(x) else x\n",
    "    )\n",
    "    \n",
    "    # Select and reorder columns\n",
    "    columns_to_keep = [\"doi\", \"material\", \"value\", \"unit\", \"passage\", \"sentence\"]\n",
    "    df = df[columns_to_keep]\n",
    "    \n",
    "    # Clean illegal control characters\n",
    "    df = df.map(clean_illegal_chars)\n",
    "\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_ce(extracted_csv, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Main function for ChatExtract post-processing.\n",
    "    \n",
    "    Args:\n",
    "        extracted_csv: Path to extracted CSV file\n",
    "        xlsx_path: Result Excel file\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    post_ce(extracted_csv, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code)\n",
    "    compare_with_index(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "codes = [\n",
    "    \"CE_1\",  # llama2:13b\n",
    "    \"CE_2\",  # llama3.1:70b\n",
    "    \"CE_3\",  # qwen2.5:14b\n",
    "]\n",
    "\n",
    "# Result data paths\n",
    "output_dir = os.path.join(HOME, \"output\", \"3-ChatExtract\")\n",
    "csvs = [\n",
    "    f'2_extracted_llama2-13b_CE_1.csv',\n",
    "    f'2_extracted_llama3.1-70b_CE_2.csv',\n",
    "    f'2_extracted_qwen2.5-14b_CE_3.csv'\n",
    "]\n",
    "\n",
    "for csv, code in zip(csvs, codes):\n",
    "    extracted_csv = os.path.join(output_dir, csv)\n",
    "    xlsx_path = os.path.join(comparison_dir, f\"3_{code}_{today}.xlsx\")\n",
    "    postprocess_ce(extracted_csv, xlsx_path, comparison_xlsx, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Process LangChain Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_lc(extracted_csv, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Convert LangChain RAG raw data to standard format.\n",
    "    \n",
    "    This function:\n",
    "    1. Reads and deduplicates raw data\n",
    "    2. Parses markdown tables from each row\n",
    "    3. Standardizes table format and merges results\n",
    "    \n",
    "    Args:\n",
    "        extracted_csv: Input CSV file path: \"output_{embed}_{infer}_{dtime}.csv\"\n",
    "        xlsx_path: Output Excel file path: \"FINAL_{embed}_{infer}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    # Read CSV and remove duplicates\n",
    "    raw_df = pd.read_csv(extracted_csv)\n",
    "    raw_df = raw_df.drop_duplicates(subset=[\"output\"], keep=\"first\")\n",
    "    \n",
    "    # Keep only rows with status == 1\n",
    "    raw_df = raw_df[raw_df['status'] == 1]\n",
    "    \n",
    "    # Initialize result DataFrame\n",
    "    result_df = pd.DataFrame(columns=[\"doi\", \"material\", \"value\", \"unit\"])\n",
    "    \n",
    "    # Process each row\n",
    "    parsed_tables = []\n",
    "    for _, row in raw_df.iterrows():\n",
    "        table = parse_markdown_table(row[\"output\"])\n",
    "        if table is not None:\n",
    "            table[\"doi\"] = row[\"doi\"]\n",
    "            parsed_tables.append(table[[\"doi\", \"material\", \"value\", \"unit\"]])\n",
    "    \n",
    "    # Merge all parsed results\n",
    "    if parsed_tables:\n",
    "        result_df = pd.concat(parsed_tables, ignore_index=True)\n",
    "    \n",
    "    # Basic processing: remove \"meV\" or \"eV\" from value column\n",
    "    result_df[\"value\"] = result_df[\"value\"].apply(\n",
    "        lambda x: x.replace(\"meV\", \"\").replace(\"eV\", \"\").strip() if pd.notna(x) else x\n",
    "    )\n",
    "    \n",
    "    # Clean illegal control characters\n",
    "    result_df = result_df.map(clean_illegal_chars)\n",
    "    \n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=\"0-raw\", index=False)\n",
    "    \n",
    "    # Remove null/none values\n",
    "    result_df = result_df[\n",
    "        ~result_df[\"value\"].str.lower().str.strip().isin([\"none\", \"nan\"]) & \n",
    "        result_df[\"value\"].notna()\n",
    "    ]\n",
    "    \n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_lc(extracted_csv, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Main function for LangChain extraction post-processing.\n",
    "    \n",
    "    Args:\n",
    "        extracted_csv: Path to extracted CSV file\n",
    "        xlsx_path: Result Excel file\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    post_lc(extracted_csv, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code)\n",
    "    compare_with_index(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "\n",
    "# Result data paths\n",
    "output_dir = os.path.join(HOME, \"output\", \"4-LangChain\")\n",
    "\n",
    "csv_files = glob.glob(os.path.join(output_dir, \"output_LC_*.csv\"))\n",
    "csv_files.sort()\n",
    "codes = [\n",
    "    f\"{os.path.basename(file).split('_')[1]}_{os.path.basename(file).split('_')[2]}\" \n",
    "    for file in csv_files\n",
    "]\n",
    "\n",
    "for extracted_csv, code in zip(csv_files, codes):\n",
    "    print(extracted_csv, code)\n",
    "    xlsx_path = os.path.join(comparison_dir, f\"4_{code}_{today}.xlsx\")\n",
    "    postprocess_lc(extracted_csv, xlsx_path, comparison_xlsx, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Process Kimi Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_kimi(extracted_xlsx, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Convert Kimi extraction raw data to standard format.\n",
    "    \n",
    "    Args:\n",
    "        extracted_xlsx: Path to Kimi output Excel file\n",
    "        xlsx_path: Output Excel file path\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(extracted_xlsx, header=None, names=[\"doi\", \"output\"])\n",
    "    \n",
    "    def path2doi(pdf_path):\n",
    "        \"\"\"Convert PDF path to DOI by removing directory and extension.\"\"\"\n",
    "        return os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    df['doi'] = df['doi'].apply(path2doi)\n",
    "    result_df = pd.DataFrame(columns=[\"doi\", \"material\", \"value\", \"sentence\"])\n",
    "    \n",
    "    # Parse markdown tables from output column\n",
    "    parsed_tables = []\n",
    "    for _, row in df.iterrows():\n",
    "        table = parse_markdown_table(row[\"output\"], third=\"sentence\")\n",
    "        if table is not None:\n",
    "            table[\"doi\"] = row[\"doi\"].replace('_', '/')\n",
    "            parsed_tables.append(table[[\"doi\", \"material\", \"value\", \"sentence\"]])\n",
    "    \n",
    "    # Merge all parsed results\n",
    "    if parsed_tables:\n",
    "        result_df = pd.concat(parsed_tables, ignore_index=True)\n",
    "    \n",
    "    # Clean illegal control characters\n",
    "    result_df = result_df.map(clean_illegal_chars)\n",
    "    \n",
    "    # Save results\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_kimi(extracted_xlsx, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"Main function for Kimi extraction post-processing.\n",
    "    \n",
    "    Args:\n",
    "        extracted_xlsx: Path to Kimi output file\n",
    "        xlsx_path: Result Excel file\n",
    "        comparison_xlsx: Comparison Excel file path\n",
    "        code: Extraction method code\n",
    "    \"\"\"\n",
    "    post_kimi(extracted_xlsx, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code)\n",
    "    compare_with_index(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "# Result data paths\n",
    "KIMI_OUT = os.path.join(HOME, \"output\", \"5-Kimi\", 'Kimi_pub_2025-06-20.xlsx')\n",
    "code = \"Kimi\"\n",
    "xlsx_path = os.path.join(comparison_dir, f\"5_{code}_{today}.xlsx\")\n",
    "\n",
    "postprocess_kimi(KIMI_OUT, xlsx_path, comparison_xlsx, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate summary of all extraction results.\"\"\"\n",
    "\n",
    "summary_xlsx = os.path.join(comparison_dir, f\"summary_{today}{dir_mark}.xlsx\")\n",
    "\n",
    "# Read summary sheet\n",
    "df = pd.read_excel(comparison_xlsx, sheet_name=\"summary\")\n",
    "original_rows = len(df)\n",
    "material_filled = df[\"other_mat\"].notna().sum()\n",
    "print(f\"Original data rows: {original_rows}\")\n",
    "print(f\"'other_mat' column filled rows: {material_filled}\")\n",
    "\n",
    "# ========== Supplement index-DOI pairs from manual sheet ==========\n",
    "# Check and add missing DOI column\n",
    "if 'doi' not in df.columns:\n",
    "    df['doi'] = ''\n",
    "\n",
    "# Read manual sheet\n",
    "try:\n",
    "    manual_df = pd.read_excel(comparison_xlsx, sheet_name=\"manual\")\n",
    "    \n",
    "    # Extract index and DOI, remove duplicates\n",
    "    if all(col in manual_df.columns for col in ['index', 'doi']):\n",
    "        manual_pairs = manual_df[['index', 'doi']].drop_duplicates()\n",
    "        \n",
    "        # Create existing index-DOI pair set\n",
    "        existing_pairs = set(zip(\n",
    "            df['index'].fillna('').astype(str),\n",
    "            df['doi'].fillna('').astype(str)\n",
    "        ))\n",
    "        \n",
    "        # Find index-DOI pairs that need to be added\n",
    "        new_rows = []\n",
    "        for _, row in manual_pairs.iterrows():\n",
    "            idx_val = row['index']\n",
    "            doi_val = row['doi']\n",
    "            # Handle NaN values and convert to strings for comparison\n",
    "            idx_str = str(idx_val) if pd.notna(idx_val) else ''\n",
    "            doi_str = str(doi_val) if pd.notna(doi_val) else ''\n",
    "            \n",
    "            if (idx_str, doi_str) not in existing_pairs:\n",
    "                # Create new row (all columns empty except index/doi)\n",
    "                new_row = {col: '' for col in df.columns}\n",
    "                new_row['index'] = idx_val\n",
    "                new_row['doi'] = doi_val\n",
    "                new_rows.append(new_row)\n",
    "        \n",
    "        # Add missing rows\n",
    "        if new_rows:\n",
    "            new_df = pd.DataFrame(new_rows)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "            print(f\"Added {len(new_rows)} rows from manual sheet\")\n",
    "        else:\n",
    "            print(\"No index-DOI pairs need to be added\")\n",
    "    else:\n",
    "        print(\"Manual sheet missing 'index' or 'doi' columns, skipping\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading manual sheet: {str(e)}, skipping\")\n",
    "# ========== End of supplementation ==========\n",
    "\n",
    "# Process data\n",
    "mask = df[\"other_mat\"].notna() & (df[\"other_mat\"] != \"\")\n",
    "modified_rows = mask.sum()\n",
    "\n",
    "print(f\"Will modify {modified_rows} rows\")\n",
    "\n",
    "df.loc[mask, \"material\"] = df.loc[mask, \"other_mat\"]\n",
    "df.loc[mask, \"value\"] = \"\"\n",
    "df.drop(columns=[\"other_mat\"], inplace=True)\n",
    "\n",
    "# Sort by index and material\n",
    "sort_columns = ['index', 'material']\n",
    "df = df.sort_values(by=[col for col in sort_columns if col in df.columns])\n",
    "\n",
    "with pd.ExcelWriter(summary_xlsx, engine=\"openpyxl\", mode='w') as writer:\n",
    "    df.to_excel(writer, sheet_name=\"summary\", index=False)\n",
    "\n",
    "print(f\"Data successfully written to: {summary_xlsx}\")\n",
    "print(f\"Final data rows: {len(df)}\")\n",
    "print(f\"Modified rows: {modified_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
