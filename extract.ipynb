{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = '/your/HOME/directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Prepare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Prepare 50 papers\n",
    "1. Download arXiv dataset JSON file from <https://www.kaggle.com/datasets/Cornell-University/arxiv?resource=download>, move to HOME directory.\n",
    "2. Run the code, get ARXIV_50_JSON\n",
    "3. Download the 50 papers in ARXIV_50_JSON\n",
    "4. Manually extracted bandgap from the 50 papers, save results to COMPARISON_XLSX (see example in comparison.xlsx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "KAGGLE_JSON = os.path.join(HOME, \"arxiv-metadata-oai-snapshot.json\")\n",
    "ARXIV_DIR = os.path.join(HOME, \"arXiv_mtrl-sci\")\n",
    "os.makedirs(ARXIV_DIR, exist_ok=True)\n",
    "ARXIV_CSV = os.path.join(HOME, \"arXiv_mtrl-sci.csv\")\n",
    "ARXIV_50_JSON = os.path.join(HOME, \"arXiv_mtrl-sci_50.json\")\n",
    "\n",
    "# 定义筛选时间范围\n",
    "START_DATE = datetime(2000, 1, 1)\n",
    "END_DATE = datetime(2024, 10, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"filter\"\"\"\n",
    "# 定义一个字典用于存储按年月分组的结果\n",
    "grouped_data = defaultdict(list)\n",
    "\n",
    "# 逐行读取 JSON 文件\n",
    "with open(KAGGLE_JSON, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # 加载一行 JSON 数据\n",
    "        entry = json.loads(line.strip())\n",
    "        \n",
    "        # 筛选条件：categories 包含 'mtrl-sci'\n",
    "        if 'mtrl-sci' in entry.get('categories', ''):\n",
    "            # 提取所需字段\n",
    "            item = {\n",
    "                'id': entry['id'],\n",
    "                'doi': entry.get('doi', None),  # 如果没有 DOI，返回 None\n",
    "                'categories': entry['categories']\n",
    "            }\n",
    "            # 提取第一个版本的创建日期并格式化\n",
    "            first_version = entry['versions'][0]['created']\n",
    "            date_v1 = datetime.strptime(first_version, '%a, %d %b %Y %H:%M:%S %Z')\n",
    "            item['date-v1'] = date_v1.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # 检查日期是否在指定范围内\n",
    "            if START_DATE <= date_v1 <= END_DATE:\n",
    "                # 按年月分组\n",
    "                year_month = date_v1.strftime('%Y-%m')\n",
    "                grouped_data[year_month].append(item)\n",
    "\n",
    "# 将分组后的数据写入 JSON 文件\n",
    "for year_month, items in grouped_data.items():\n",
    "    # 获取条目数量\n",
    "    count = len(items)\n",
    "    # 在文件名中添加条目数量\n",
    "    output_file = os.path.join(ARXIV_DIR, f\"{year_month}({count}).json\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        json.dump(items, out_f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"randonly choose 50 papers\"\"\"\n",
    "# 1. 读取json文件\n",
    "json_files = sorted(glob(os.path.join(ARXIV_DIR, '*.json')))\n",
    "\n",
    "data = []\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as f:\n",
    "        content = json.load(f)\n",
    "        for paper in content:\n",
    "            # 提取年份\n",
    "            year = paper['date-v1'][:4] if paper['date-v1'] else None\n",
    "            data.append({\n",
    "            'year': year,\n",
    "            'id': paper['id'],\n",
    "            'doi': paper['doi']\n",
    "        })\n",
    "\n",
    "# 2. 创建DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 3. 统计独立的id和doi数量\n",
    "unique_ids = df['id'].nunique()\n",
    "unique_dois = df['doi'].dropna().nunique()  # 排除None值后统计\n",
    "\n",
    "print(f\"独立的文章ID数量: {unique_ids}\")\n",
    "print(f\"独立的DOI数量: {unique_dois}\")\n",
    "\n",
    "# 4. 保存为CSV文件\n",
    "df.to_csv(ARXIV_CSV, index=False)\n",
    "print(f\"\\n数据已保存到: {ARXIV_CSV}\")\n",
    "\n",
    "# 5. 随机选取50篇文章\n",
    "# 只保留同时包含id和doi的文章\n",
    "df_complete = df.dropna(subset=['id', 'doi'])\n",
    "df_sample = df_complete.sample(n=50, random_state=42)\n",
    "df_sample.to_json(ARXIV_50_JSON, orient='records', indent=2)\n",
    "print(f\"已将50篇文章样本保存到: {ARXIV_50_JSON}\")\n",
    "\n",
    "# 6. 统计年份分布\n",
    "year_distribution = df_sample['year'].value_counts().sort_index()\n",
    "print(\"\\n年份分布情况:\")\n",
    "print(year_distribution)\n",
    "\n",
    "# 可视化年份分布\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "year_distribution.plot(kind='bar')\n",
    "plt.title('Sample Articles Year Distribution')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Sparse PDF to TXT\n",
    "\n",
    "1. Download the 50 papers' PDF files to PDF_DIR\n",
    "2. Run the code to get TXT_DIR_2 (TXT files, with papers in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "\n",
    "PDF_DIR = os.path.join(HOME, \"PDF\")\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "TXT_DIR_1 = os.path.join(HOME, \"TXT(fromPDF)\")\n",
    "os.makedirs(TXT_DIR_1, exist_ok=True)\n",
    "TXT_DIR_2 = os.path.join(HOME, \"TXT(fromPDF_processed)\")\n",
    "os.makedirs(TXT_DIR_2, exist_ok=True)\n",
    "\n",
    "\"\"\"Sparse\"\"\"\n",
    "# 遍历每个pdf，将每个pdf的text输出到txt文件中，保存在TXT_DIR目录下，文件名与pdf文件名一致\n",
    "pdf_files = sorted(glob.glob(os.path.join(PDF_DIR, \"*.pdf\")))\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyMuPDFLoader(pdf_file)\n",
    "    docs = loader.load()\n",
    "    text = \"\"\n",
    "    for doc in docs:\n",
    "        text += doc.page_content\n",
    "    with open(os.path.join(TXT_DIR_1, os.path.basename(pdf_file).replace('.pdf', '.txt')), 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Process\"\"\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 常量定义\n",
    "EXCLUDED_ENDINGS = ('Fig.', 'Eq.', 'Figs.', 'et al.')\n",
    "SENTENCE_ENDINGS = ('.', '!', '?')\n",
    "MIN_SENTENCE_LENGTH = 20\n",
    "\n",
    "def should_merge(sentence: str) -> bool:\n",
    "    \"\"\"判断是否需要合并下一个句子\"\"\"\n",
    "    if not sentence:\n",
    "        return False\n",
    "    # 如果不以标准句子结尾符结束，或者以排除项结尾，则需要合并\n",
    "    return (not sentence.endswith(SENTENCE_ENDINGS) \n",
    "            or any(sentence.endswith(end) for end in EXCLUDED_ENDINGS))\n",
    "\n",
    "def merge_consecutive(sentences: list[str]) -> list[str]:\n",
    "    \"\"\"合并需要连接的连续句子\"\"\"\n",
    "    merged = []\n",
    "    i, n = 0, len(sentences)\n",
    "    \n",
    "    while i < n:\n",
    "        current = sentences[i].strip()\n",
    "        j = i + 1\n",
    "        \n",
    "        # 连续合并需要连接的句子\n",
    "        while j < n and should_merge(current):\n",
    "            current += \" \" + sentences[j].strip()\n",
    "            j += 1\n",
    "        \n",
    "        merged.append(current)\n",
    "        i = j  # 移动到下一个未处理的句子\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def merge_short_sentences(sentences: list[str]) -> list[str]:\n",
    "    \"\"\"合并过短的句子到前一句\"\"\"\n",
    "    merged = []\n",
    "    for sentence in sentences:\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        # 如果当前句子过短且结果列表不为空，则合并到前一句\n",
    "        if merged and len(sentence) < MIN_SENTENCE_LENGTH:\n",
    "            merged[-1] += \" \" + sentence\n",
    "        else:\n",
    "            merged.append(sentence)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def process_txt(input_path: str, output_path: str) -> None:\n",
    "    # 读取并预处理文本\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_text = file.read().replace(\"\\n\", \" \")\n",
    "    \n",
    "    # 初始分句\n",
    "    doc = nlp(raw_text)\n",
    "    initial_sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    # 多阶段处理流程\n",
    "    processed = merge_consecutive(initial_sentences)\n",
    "    processed = merge_short_sentences(processed)\n",
    "    processed = merge_consecutive(processed)  # 处理短句合并后的新情况\n",
    "    \n",
    "    # 写入结果\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(processed))\n",
    "\n",
    "# 读取OUTPUT1_DIR下的所有txt，使用process_txt依次处理后，保存至OUTPUT2_DIR\n",
    "for txt_file in glob.glob(os.path.join(TXT_DIR_1, \"*.txt\")):\n",
    "    process_txt(txt_file, os.path.join(TXT_DIR_2, os.path.basename(txt_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Extraction\n",
    "\n",
    "1. Download projects to PROJECT_DIR\n",
    "   1. <https://github.com/QingyangDong-qd220/BandgapDatabase1>\n",
    "   2. <https://github.com/StefanoSanvitoGroup/BERT-PSIE-TC>\n",
    "2. Download LLMs\n",
    "   1. <https://huggingface.co/m3rg-iitd/matscibert>\n",
    "   2. <https://ollama.com/library/nomic-embed-text> (ollama pull nomic-embed-text)\n",
    "   3. <https://ollama.com/library/bge-m3> (ollama pull bge-m3)\n",
    "   4. <https://ollama.com/library/llama2> (ollama pull llama2:13b)\n",
    "   5. <https://huggingface.co/bartowski/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF> (ollama create llama3.1:70b -f Modelfile)\n",
    "   6. <https://ollama.com/library/qwen2.5> (ollama pull qwen2.5:14b)\n",
    "3. Use PROMPT below to extract from Kimi and place the output to KIMI_OUT\n",
    "\n",
    "\n",
    "\n",
    "PROMPT:\n",
    "\n",
    "You are an expert information extraction algorithm.\n",
    "Extract all the band gap values in this article and output them in the form of a markdown table, including: Material (name of the material), Value (value with unit), Sentence (the sentence from which this data record comes).\n",
    "If data is not present in the article, type \"None\". \n",
    "Table only, no need for explanation or any other content.\n",
    "The output is strictly in the following format.\n",
    "```markdown\n",
    "| Material | Value | Sentence |\n",
    "|----------|-------|---------|\n",
    "| Material1 | 0.1 eV | ... Eg of Material1 is 0.1 eV ... |\n",
    "| Material1 | 200 meV | Material1 has a band gap of 200 meV, so ... |\n",
    "| Material2 | None | Material2 ... |\n",
    "```\n",
    "\n",
    "If no band gap values mentioned in the article, the following table is acceptable:\n",
    "```markdown\n",
    "| Material | Value | Sentence |\n",
    "|----------|-------|----------|\n",
    "| None | None | None |\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-CDE\n",
    "docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**docker 前期准备**\n",
    "\n",
    "chemdataextractor2 无法导入 ➡️ 把\"/usr/local/lib/python3.8/site-packages/chemdataextractor\"改为\"chemdataextractor2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playsound openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"extract\"\"\"\n",
    "import os\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from chemdataextractor2.relex import Snowball\n",
    "from chemdataextractor2.model.units.energy import EnergyModel\n",
    "from chemdataextractor2.model import BaseModel, StringType, ListType, ModelType, Compound\n",
    "from chemdataextractor2.parse import R, I, W, Optional, merge, join, AutoSentenceParser\n",
    "from chemdataextractor2.doc import Sentence, Document\n",
    "\n",
    "class BandGap(EnergyModel):\n",
    "    specifier_expression = (\n",
    "        (I(\"band\") + R(\"gaps?\")) | I(\"bandgap\") | I(\"band-gap\") | I(\"Eg\")\n",
    "    ).add_action(join)\n",
    "    specifier = StringType(\n",
    "        parse_expression=specifier_expression, required=True, updatable=True\n",
    "    )\n",
    "    compound = ModelType(\n",
    "        Compound, required=True, contextual=True, binding=True, updatable=False\n",
    "    )\n",
    "    parsers = [AutoSentenceParser()]\n",
    "\n",
    "def run(file_path, article):\n",
    "    \"\"\"\n",
    "    use snowball to serialize an article\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    # load a paper\n",
    "    try:\n",
    "        d = Document.from_file(file_path)\n",
    "    except:\n",
    "        print(\"unable to read document\")\n",
    "        return\n",
    "\n",
    "    publisher = \"arXiv\"\n",
    "\n",
    "    # process a paper\n",
    "    for p in d.paragraphs:\n",
    "        for s in p.sentences:\n",
    "            if s.end - s.start > 300:\n",
    "                continue\n",
    "\n",
    "            results_snow = []\n",
    "            results_auto = []\n",
    "            snow_85 = False\n",
    "\n",
    "            \"\"\"nwk: 第一种提取，使用 AutoSentenceParser，输出在 results_auto\"\"\"\n",
    "            # auto\n",
    "            BandGap.parsers = [AutoSentenceParser()]\n",
    "            s.models = [BandGap]  # nwk: 提取\n",
    "            auto = s.records.serialize()  # nwk: 解析\n",
    "            for i in auto:\n",
    "                if \"BandGap\" in i.keys():  # nwk: 存在带隙相关数据\n",
    "                    if (\n",
    "                        \"raw_value\" in i[\"BandGap\"].keys()\n",
    "                        and \"compound\" in i[\"BandGap\"].keys()\n",
    "                    ):  # nwk: 存在带隙值、材料名\n",
    "                        if \"names\" in i[\"BandGap\"][\"compound\"][\"Compound\"].keys():\n",
    "                            i[\"BandGap\"][\n",
    "                                \"text\"\n",
    "                            ] = s.text  # nwk: 在 text 条目储存原始的输出？\n",
    "                            i['BandGap']['doi'] = article.replace('_', '/').replace('.html', '').replace('.xml', '').replace('.txt', '')  # nwk: 提取文件名中的 doi（所以文件名需要是 doi）\n",
    "                            results_auto.append(i)\n",
    "\n",
    "            \"\"\"nwk: 第二种提取，使用 snowball，输出在 results_snow\"\"\"\n",
    "            # snow\n",
    "            snowball.minimum_cluster_similarity_score = 0.85\n",
    "            BandGap.parsers = [snowball]\n",
    "            s.models = [BandGap]  # nwk: 提取\n",
    "            snow = s.records.serialize()  # nwk: 解析\n",
    "            for i in snow:\n",
    "                if \"BandGap\" in i.keys():  # nwk: 这里只要求有 bandgap 相关的条目\n",
    "                    snow_85 = True\n",
    "                    i[\"BandGap\"][\"text\"] = s.text\n",
    "                    i['BandGap']['doi'] = article.replace('_', '/').replace('.html', '').replace('.xml', '').replace('.txt', '')\n",
    "                    results_snow.append(i)\n",
    "\n",
    "            if snow_85 == False:  # nwk: 如果“minimum_cluster_similarity_score”参数为 0.85 提取不到数据时，换用 0.65\n",
    "                snowball.minimum_cluster_similarity_score = 0.65\n",
    "                BandGap.parsers = [snowball]\n",
    "                s.models = [BandGap]\n",
    "                snow = s.records.serialize()\n",
    "                for i in snow:\n",
    "                    if \"BandGap\" in i.keys():\n",
    "                        i[\"BandGap\"][\"text\"] = s.text\n",
    "                        i['BandGap']['doi'] = article.replace('_', '/').replace('.html', '').replace('.xml', '').replace('.txt', '')\n",
    "                        results_snow.append(i)\n",
    "\n",
    "            \"\"\"nwk: 提取完成，将 results_snow 整合进 results_auto\"\"\"\n",
    "            # combine results from Snowball to AutoSentenceParser\n",
    "            for i in results_auto:\n",
    "                i[\"BandGap\"][\"AutoSentenceParser\"] = 1\n",
    "                i[\"BandGap\"][\"Snowball\"] = 0\n",
    "                for j in range(len(results_snow)):\n",
    "                    if i['BandGap']['compound']['Compound']['names'] == results_snow[j]['BandGap']['compound']['Compound']['names']:  # nwk: 整合相同的材料\n",
    "                        i[\"BandGap\"] = results_snow[j][\"BandGap\"]  # nwk: 合并\n",
    "                        i[\"BandGap\"][\"Snowball\"] = 1\n",
    "                        i[\"BandGap\"][\"AutoSentenceParser\"] = 1\n",
    "                        results_snow[j][\"BandGap\"][\"match\"] = 1  # nwk: 标记\n",
    "                        continue\n",
    "\n",
    "            \"\"\"nwk: 通过上一步的 match 标记，将 auto 没有提取到的数据 results_snow 添加进 results_auto\"\"\"\n",
    "            # Snowball only results\n",
    "            for x in results_snow:\n",
    "                if \"match\" not in x[\"BandGap\"].keys():\n",
    "                    x[\"BandGap\"][\"Snowball\"] = 1\n",
    "                    x[\"BandGap\"][\"AutoSentenceParser\"] = 0\n",
    "                    results_auto.append(x)\n",
    "\n",
    "            \"\"\"nwk: 最后给条目添加出版商信息\"\"\"\n",
    "            if results_auto:\n",
    "                for i in results_auto:\n",
    "                    i[\"BandGap\"][\"publisher\"] = publisher\n",
    "                    results.append(i)\n",
    "\n",
    "    return results\n",
    "\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "HOME_DOCKER = \"/home/chemdataextractor2/output/HOME_0325\"\n",
    "PROJECT_DIR = os.path.join(HOME_DOCKER, \"project\")\n",
    "TXT_DIR = os.path.join(HOME_DOCKER, \"TXT(fromPDF_processed)\")\n",
    "OUTPUT_DIR = os.path.join(HOME_DOCKER, \"output\", \"1-ChemDataExtractor\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "TEMP_SAVE = os.path.join(OUTPUT_DIR, \"records_general.joblib\")\n",
    "RUNTIME = os.path.join(OUTPUT_DIR, f'runtime_{dtime}.txt')\n",
    "\n",
    "SNOWBALL_PATH = os.path.join(PROJECT_DIR, \"BandgapDatabase1-main\", \"Snowball_model\", \"general.pkl\")\n",
    "\n",
    "\"\"\"load snowball model\"\"\"\n",
    "snowball = Snowball.load(SNOWBALL_PATH)\n",
    "snowball.minimum_relation_confidence = 0.001\n",
    "snowball.max_candidate_combinations = 100\n",
    "snowball.save_file_name = \"general\"  # model_name\n",
    "snowball.set_learning_rate(0.0)\n",
    "\n",
    "# load already found records\n",
    "try:\n",
    "    records = joblib.load(TEMP_SAVE)\n",
    "    print(\"load existing records\")\n",
    "except:\n",
    "    records = []\n",
    "    print(\"no records found\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for file_name in tqdm(os.listdir(TXT_DIR), desc=\"Processing files\"):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(TXT_DIR, file_name)\n",
    "        temp = run(file_path, file_name)\n",
    "        # save records\n",
    "        if temp:\n",
    "            pprint(temp)\n",
    "            for record in temp:\n",
    "                records.append(record)\n",
    "            joblib.dump(records, TEMP_SAVE)\n",
    "\n",
    "end_time = datetime.now()\n",
    "run_time = end_time - start_time\n",
    "\n",
    "with open(RUNTIME, 'w') as f:\n",
    "    f.write(f'Total runtime: {run_time}')\n",
    "\n",
    "# took 3:37:29(217m 30.1s)\n",
    "# took (233m 9.0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"postprocessing(kernel: lc)\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from my_post import clean_and_normalize, compare\n",
    "\n",
    "def post_cde(temp_save, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"ChemDataExtractor提取的原始数据转换为标准格式\n",
    "    \n",
    "    Args:\n",
    "        temp_save: CDE提取的原始数据文件路径\n",
    "        xlsx_path: 输出的Excel文件路径: \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "    \"\"\"\n",
    "    records = joblib.load(temp_save)\n",
    "    # 提取需要的字段\n",
    "    columns = [\"Publisher\", \"DOI\", \"Name\", \"Raw_value\", \"Raw_unit\", \"Value\", \"Unit\", \n",
    "              \"specifier\", \"Text\", \"Snowball\", \"AutoSentenceParser\"]\n",
    "    flat_data = []\n",
    "    for item in records:\n",
    "        bandgap = item[\"BandGap\"]\n",
    "        flat_item = {\n",
    "            \"Publisher\": bandgap[\"publisher\"],\n",
    "            \"DOI\": bandgap[\"doi\"],\n",
    "            \"Name\": bandgap[\"compound\"][\"Compound\"][\"names\"][0],\n",
    "            \"Raw_value\": bandgap[\"raw_value\"],\n",
    "            \"Raw_unit\": bandgap[\"raw_units\"], \n",
    "            \"Value\": bandgap[\"value\"],\n",
    "            \"Unit\": bandgap[\"units\"],\n",
    "            \"specifier\": bandgap[\"specifier\"],\n",
    "            \"Text\": bandgap[\"text\"],\n",
    "            \"Snowball\": bandgap[\"Snowball\"],\n",
    "            \"AutoSentenceParser\": bandgap[\"AutoSentenceParser\"]\n",
    "        }\n",
    "        flat_data.append(flat_item)\n",
    "    # 保存原始数据\n",
    "    df = pd.DataFrame(flat_data, columns=columns)\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"0-raw\", index=False)\n",
    "    # 保存简化数据\n",
    "    simplified_df = pd.DataFrame({\n",
    "        'doi': df['DOI'],\n",
    "        'material': df['Name'],\n",
    "        'value': df['Raw_value'], \n",
    "        'unit': df['Raw_unit']\n",
    "    })\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        simplified_df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        simplified_df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_cde(temp_save, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"CDE提取流程的主函数\n",
    "    \n",
    "    Args:\n",
    "        temp_save: CDE提取的原始数据文件路径（joblib文件）\n",
    "        xlsx_path: 结果Excel - \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: 标准答案文件路径\n",
    "        code: 提取方法代码\n",
    "    \"\"\"\n",
    "    post_cde(temp_save, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code, sheet_name=\"1-raw\")\n",
    "    compare(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "\"\"\"此处的路径是docker之外的\"\"\"\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "OUTPUT_DIR = os.path.join(HOME, \"output\", \"1-ChemDataExtractor\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "TEMP_SAVE = os.path.join(OUTPUT_DIR, \"records_general.joblib\")\n",
    "COMPARISON_XLSX = os.path.join(HOME, \"comparison.xlsx\")\n",
    "CODE = \"CDE\"\n",
    "XLSX_PATH = os.path.join(HOME, f\"1_{CODE}_{dtime}.xlsx\")\n",
    "\n",
    "postprocess_cde(TEMP_SAVE, XLSX_PATH, COMPARISON_XLSX, CODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-PSIE\n",
    "kernel: lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"extract\"\"\"\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import BertTokenizerFast\n",
    "from seqeval.metrics import classification_report  # 用于评估序列标注任务的性能\n",
    "import nltk  # 用于自然语言处理任务\n",
    "import re\n",
    "from pymatgen.core import Composition  # 用于处理化学式和晶体结构\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATA_DIR = os.path.join(HOME, \"TXT(fromPDF_processed)\")\n",
    "# model\n",
    "PROJECT_DIR = os.path.join(HOME, \"project\")\n",
    "PSIE_DIR = os.path.join(PROJECT_DIR, \"BERT-PSIE-TC-main\", \"workflow\")\n",
    "import sys\n",
    "sys.path.insert(1, PSIE_DIR)\n",
    "import psie\n",
    "MODEL_DIR = os.path.join(PSIE_DIR, \"models\", \"Gap\")\n",
    "CLASSIFIER_PATH = os.path.join(MODEL_DIR, \"classifier.pt\")\n",
    "NER_PATH = os.path.join(MODEL_DIR, \"ner\")\n",
    "RELATION_PATH = os.path.join(MODEL_DIR, \"relation\")\n",
    "BERT_VERSION = \"/Volumes/External/model_cache/huggingface/models--m3rg-iitd--matscibert/snapshots/24a4e4318dda9bc18bff5e6a45debdcb3e1780e3\"\n",
    "MAX_LEN = 256  # 文本序列的最大长度为256\n",
    "\n",
    "# output\n",
    "OUTPUT_DIR = os.path.join(HOME, \"output\", \"2-BERT-PSIE\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "SENTENCES_JSON = os.path.join(OUTPUT_DIR, \"sentences.json\")\n",
    "OUTPUT_JSON_1 = os.path.join(OUTPUT_DIR, \"1-relevant_sentences.json\")\n",
    "OUTPUT_JSON_2_M = os.path.join(OUTPUT_DIR, \"2-test_extraction_multiple_mentions.json\")\n",
    "OUTPUT_CSV_2_S = os.path.join(OUTPUT_DIR, \"2-test_extraction_single_mentions.csv\")\n",
    "OUTPUT_CSV_3 = os.path.join(OUTPUT_DIR, \"3-relations_extraction.csv\")\n",
    "RUNTIME = os.path.join(OUTPUT_DIR, f'runtime_{dtime}.txt')\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "start_time_total = time.time()\n",
    "\n",
    "\"\"\"process sentences to json\"\"\"\n",
    "# 定义函数来处理文本文件并生成句子列表\n",
    "def process_txt_files(data_dir):\n",
    "    sentences_list = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            doi = filename[:-4]  # 移除 .txt 后缀\n",
    "            with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    sentence = line.strip()\n",
    "                    if sentence:\n",
    "                        sentences_list.append({\"sentence\": sentence, \"source\": doi})\n",
    "    return sentences_list\n",
    "\n",
    "# 处理文本文件并生成句子列表\n",
    "sentences = process_txt_files(DATA_DIR)\n",
    "# 将句子列表写入 JSON 文件\n",
    "with open(SENTENCES_JSON, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(sentences, json_file, ensure_ascii=False, indent=2)\n",
    "print(f\"已将 {len(sentences)} 个句子写入 {SENTENCES_JSON}\")\n",
    "\n",
    "\"\"\"1/3 Classifier\"\"\"\n",
    "start_time_classifier = time.time()\n",
    "\n",
    "'''tokenize'''\n",
    "dataset = load_dataset(path=\"json\", data_files=SENTENCES_JSON, split=\"train\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_VERSION)  # 使用MatSciBERT\n",
    "\n",
    "def encode(paper):  # 使用tokenizer对paper中的句子进行分词、编码和填充处理\n",
    "  return tokenizer(paper[\"sentence\"], truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)  # 以“批处理模式”对数据集的每一个数据应用encode函数\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"source\", \"sentence\", \"input_ids\", \"attention_mask\"])  # 设置数据集的格式为PyTorch格式，选择需要的列\n",
    "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)  # 创建一个数据加载器，用于按批次加载数据集，每批次大小为32，不打乱数据顺序\n",
    "\n",
    "\n",
    "'''classify'''\n",
    "model = psie.classifier.BertClassifier()  # 实例化psie模块中的BertClassifier类，创建一个BERT分类模型\n",
    "# model.load_state_dict(torch.load(MODEL_PATH), device=device)  # 加载预训练的模型参数\n",
    "state_dict = torch.load(CLASSIFIER_PATH, map_location=device)\n",
    "state_dict.pop(\"bert.embeddings.position_ids\", None)  # 删除不需要的键\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)  # 将模型移到GPU上\n",
    "\n",
    "pred = model.predict(dataset_loader, device)  # 对数据集应用分类模型，得到分类结果\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(pred)):\n",
    "    predictions.append(np.argmax(pred[i].cpu().numpy()))  # 概率最大的类别索引\n",
    "\n",
    "# 将过滤得到的“相关句子”存为json文件（包含句子和doi）\n",
    "filtered_sentences = {\"sentence\": [], \"source\":[]}\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "  if predictions[i] == 1:\n",
    "    filtered_sentences[\"sentence\"].append((dataset[i][\"sentence\"]))\n",
    "    filtered_sentences[\"source\"].append((dataset[i][\"source\"]))\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON_1), exist_ok=True)\n",
    "with open(OUTPUT_JSON_1, \"w\") as f:\n",
    "    json.dump(filtered_sentences, f)\n",
    "\n",
    "classifier_time = time.time() - start_time_classifier\n",
    "\n",
    "\"\"\"2/3 NER\"\"\"\n",
    "start_time_ner = time.time()\n",
    "\n",
    "# 设置实体标签的映射关系\n",
    "id_to_BOI = {\n",
    "    1: \"B-CHEM\",  # Chemical entity\n",
    "    0: \"O\",  # No entity\n",
    "    2: \"B-BANDGAP\"\n",
    "}\n",
    "with open(OUTPUT_JSON_1, \"r\") as f:\n",
    "    data = json.load(f)  # 加载JSON数据\n",
    "tokenizer = BertTokenizerFast.from_pretrained(NER_PATH)  # 使用预训练的NER模型\n",
    "\n",
    "sentences = psie.NerUnlabeledDataset(data[\"sentence\"], tokenizer, max_len=MAX_LEN)  # 传入语料库中的句子数据、分词器和最大长度参数\n",
    "sources = data[\"source\"]  # 获取语料库中的数据源信息\n",
    "sentences_params = {'batch_size': 10,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 0\n",
    "}\n",
    "sentences_loader = DataLoader(sentences, **sentences_params)  # 创建一个数据加载器，用于加载句子数据\n",
    "\n",
    "model = psie.BertForNer.from_pretrained(NER_PATH, num_labels=3)  # 实例化预训练的 NER 模型，标签数量为3\n",
    "model.to(device)\n",
    "# NER predictions\n",
    "predictions = model.predict(sentences_loader, device, id_to_BOI)\n",
    "\n",
    "# 对每个句子的预测结果进行处理，提取其中的实体标签。首先对句子进行分词和预处理，然后根据预测结果和分词结果提取实体标签，并将结果保存到extr_labels列表中。\n",
    "extr_labels = []\n",
    "for n in range(len(predictions)):\n",
    "    tokens = tokenizer.tokenize(\n",
    "        \"[CLS]\" + psie.preprocess_text(sentences[n][\"plain\"]) + \"[SEP]\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    extracted = {}\n",
    "    i = 0\n",
    "    while i < MAX_LEN:\n",
    "        if predictions[n][i] != \"O\" and tokens[i] not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            entity = predictions[n][i]\n",
    "            entry = []\n",
    "            while predictions[n][i] == entity:\n",
    "                entry.append(tokens[i])\n",
    "                i += 1\n",
    "                if i >= MAX_LEN:\n",
    "                    break\n",
    "            if entity in extracted.keys():\n",
    "                extracted[entity].append(\" \".join(entry))\n",
    "            else:\n",
    "                extracted[entity] = [\" \".join(entry)]\n",
    "        i += 1\n",
    "    extr_labels.append(extracted)\n",
    "\n",
    "'''multiple mentioned'''\n",
    "# Extract the sentences with multiple mentions of Chem and Tc/Gap\n",
    "# The extracted sentences are saved in a json file and will be processed by the BERT model finetuned for relation classification.\n",
    "#\n",
    "relational = []\n",
    "\n",
    "for i in range(len(extr_labels)):\n",
    "  n_entries = [len(extr_labels[i][key]) for key in extr_labels[i].keys()]\n",
    "  if n_entries != []:\n",
    "    if len(n_entries) == 2:\n",
    "      if n_entries[0] > 1 and n_entries[1] > 1:\n",
    "        relational.append(extr_labels[i].copy())\n",
    "\n",
    "        relational[-1][\"sentence\"] = sentences[i][\"plain\"]\n",
    "        relational[-1][\"source\"] = sources[i]\n",
    "\n",
    "print(\"Relational/Total: \", len(relational), \"/\", len(predictions))\n",
    "with open(OUTPUT_JSON_2_M, \"w\") as f:\n",
    "    json.dump(relational, f)\n",
    "\n",
    "'''single mentioned'''\n",
    "# Extract the sentences with exactly 1 mention of Chem and 1 mention of Tc/Gap\n",
    "relevant = []\n",
    "\n",
    "for i in range(len(extr_labels)):\n",
    "    n_entries = [len(extr_labels[i][key]) for key in extr_labels[i].keys()]\n",
    "    if n_entries == [1, 1]:\n",
    "        relevant.append(extr_labels[i])\n",
    "\n",
    "        relevant[-1][\"sentence\"] = sentences[i][\"plain\"]\n",
    "        relevant[-1][\"source\"] = sources[i]\n",
    "\n",
    "print(\"Relevant/Total: \", len(relevant), \"/\", len(predictions))\n",
    "\n",
    "# Cleaning of the sentences with single mentions of CHEM and Tc/Gap at every step the sentences that raise an exception are printed for debugging purpose\n",
    "# 对上面的单对“化合物-性质”的句子进行清洗（整理），储存到database字典中，并把异常的句子打印出来以供调试\n",
    "database = {\"compound\": [], \"Gap\": [], \"sentence\": [], \"source\": []}\n",
    "\n",
    "for n in range(len(relevant)):\n",
    "    chem, trgt = None, None\n",
    "\n",
    "    try:\n",
    "        chem = (\n",
    "            relevant[n][\"B-CHEM\"][0]\n",
    "            .strip()\n",
    "            .replace(\" \", \"\")\n",
    "            .replace(\"#\", \"\")\n",
    "            .replace(\"(\", \"\\(\")\n",
    "            .replace(\")\", \"\\)\")\n",
    "            .replace(\"+\", \"\\+\")\n",
    "            .replace(\"[UNK]\", \"\")\n",
    "            .replace(\".\", \"\\.\")\n",
    "        )\n",
    "\n",
    "        chem = re.findall(\n",
    "            \"(?i)[^a-zA-Z0-9]*\" + chem + \"[^a-zA-Z]\",\n",
    "            relevant[n][\"sentence\"],\n",
    "        )[0].strip()\n",
    "\n",
    "        if chem.endswith(\",\") or chem.endswith(\".\"):\n",
    "            chem = chem[0 : len(chem) - 1]\n",
    "        if chem.startswith(\",\") or chem.startswith(\".\"):\n",
    "            chem = chem[1 : len(chem)]\n",
    "\n",
    "        if chem in psie.ELEMENT_NAMES:\n",
    "            chem = psie.ELEMENTS[psie.ELEMENT_NAMES.index(chem)]\n",
    "\n",
    "        trgt = relevant[n][id_to_BOI[2]][0].replace(\"#\", \"\").strip()\n",
    "        trgt = (\n",
    "            trgt.replace(\"[\", \"\")\n",
    "            .replace(\"]\", \"\")\n",
    "            .replace(\"{\", \"\")\n",
    "            .replace(\"}\", \"\")\n",
    "            .replace(\"=\", \"\")\n",
    "            .replace(\"[UNK]\", \"\")\n",
    "        )\n",
    "\n",
    "        trgt = trgt.replace(\"ev\", \"eV\")\n",
    "\n",
    "        if trgt.endswith(\",\") or trgt.endswith(\".\"):\n",
    "            trgt = trgt[0 : len(trgt) - 1]\n",
    "        if trgt.startswith(\",\") or trgt.startswith(\".\"):\n",
    "            trgt = trgt[1 : len(trgt)]\n",
    "\n",
    "        if (chem is not None) and (trgt is not None):\n",
    "            database[\"compound\"].append(chem)\n",
    "            database[\"Gap\"].append(trgt)\n",
    "\n",
    "        database[\"sentence\"].append(relevant[n][\"sentence\"])\n",
    "        database[\"source\"].append(relevant[n][\"source\"])\n",
    "\n",
    "    except:\n",
    "        comp = (\n",
    "            relevant[n][\"B-CHEM\"][0]\n",
    "            .replace(\"#\", \"\")\n",
    "            .replace(\" \", \"\")\n",
    "            .replace(\"(\", \"\\(\")\n",
    "            .replace(\")\", \"\\)\")\n",
    "            .replace(\"+\", \"\\+\")\n",
    "            .replace(\"[UNK]\", \"\")\n",
    "        )\n",
    "        trgt = relevant[n][id_to_BOI[2]][0].replace(\"#\", \"\").strip()\n",
    "        print(comp, trgt, relevant[n][\"sentence\"], \"\\n\\n\")  ### Print the cases that raise an exception (for debugging purposes)\n",
    "# 清洗后的条目数 / 原始句子数\n",
    "print(\"Database entries:\", len(database[\"compound\"]), \"/\", len(relevant))\n",
    "\n",
    "# The chemical entity is converted to a Composition object from pymatgen and its reduced formula is taken\n",
    "database = pd.DataFrame(database)  # 将清洗后的数据转为DataFrame\n",
    "\n",
    "valid_i = []\n",
    "\n",
    "for i, mat in enumerate(database[\"compound\"]):\n",
    "    try:\n",
    "        Composition(mat).get_reduced_formula_and_factor()[\n",
    "            0\n",
    "        ]  # 使用pymatgen库将化学实体转换为Composition对象，并获取其简化化学式\n",
    "        valid_i.append(i)\n",
    "    except:\n",
    "        print(\n",
    "            mat, \"\\t\", database[\"sentence\"][i], \"\\n\\n\"\n",
    "        )  # The entries that raise an exception are printed for debugging purpose\n",
    "# 通过验证的条目数 / 原始数据中的句子总数\n",
    "print(\"Database entries:\", len(valid_i), \"/\", len(relevant))\n",
    "# 将通过验证的条目导出为CSV文件\n",
    "database.iloc[valid_i].to_csv(OUTPUT_CSV_2_S)\n",
    "\n",
    "ner_time = time.time() - start_time_ner\n",
    "\n",
    "\"\"\"3/3 Relation\"\"\"\n",
    "start_time_relation = time.time()\n",
    "\n",
    "# Adding the tokens for the relation extraction step to the BERT models vocabulary so that this tags are not splitted into different subwords.将关系提取步骤的标记添加到 BERT 模型词汇中，这样标记就不会被分割成不同的子词。\n",
    "tokenizer = BertTokenizerFast.from_pretrained(RELATION_PATH)\n",
    "new_tokens = [\"[E1]\", \"[/E1]\", \"[E2]\", \"[/E2]\"]\n",
    "tokenizer.add_tokens(list(new_tokens))  # 添加关系提取步骤的新标记\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "with open(OUTPUT_JSON_2_M, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(data[0])\n",
    "\n",
    "\n",
    "# 给实体（材料和gap）的前后添加标签\n",
    "data = psie.fromNer(data)\n",
    "\n",
    "print(data[\"sentence\"][0])\n",
    "print(data[\"sentence\"][1])\n",
    "\n",
    "\n",
    "# 如果句子同时包含[E1]和[E2]，则填充ner_dataset\n",
    "ner_dataset = {\"sentence\": [], \"isrelated\": [], \"source\": []}\n",
    "for i in range(len(data[\"sentence\"])):\n",
    "    if (\"[E1]\" in data[\"sentence\"][i]) and (\"[E2]\" in data[\"sentence\"][i]):\n",
    "        ner_dataset[\"sentence\"].append(str(data[\"sentence\"][i]))\n",
    "        ner_dataset[\"isrelated\"].append(None)\n",
    "        ner_dataset[\"source\"].append(data[\"source\"][i])\n",
    "print(len(ner_dataset[\"sentence\"]), \"/\", len(data[\"sentence\"]))\n",
    "\n",
    "ner = psie.RelationDataset(\n",
    "    ner_dataset, tokenizer, max_len=MAX_LEN\n",
    ")  # 使用psie.RelationDataset将数据转换为模型可接受的格式\n",
    "ner_params = {\"batch_size\": 8, \"shuffle\": False, \"num_workers\": 0}\n",
    "ner_loader = DataLoader(ner, **ner_params)  # 按要求加载数据\n",
    "\n",
    "model = psie.BertForRelations(\n",
    "    pretrained=RELATION_PATH, dropout=0.2, use_cls_embedding=True\n",
    ")\n",
    "model.bert.resize_token_embeddings(len(tokenizer))  # 调整模型的嵌入层大小以适应新的标记数\n",
    "model.to(device)\n",
    "\n",
    "# Predictions on the BERT NER output\n",
    "pred = model.predict(ner_loader, device)  # NER预测\n",
    "predictions = []  # 将每个预测结果的最大值索引添加到predictions列表中\n",
    "for i in range(len(pred)):\n",
    "    predictions.append(np.argmax(pred[i].cpu().numpy()))\n",
    "\n",
    "# 存储提取的关系数据\n",
    "database = {\"compound\": [], \"Gap\": [], \"sentence\": [], \"source\": []}\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == 1:\n",
    "        # 材料\n",
    "        comp = re.findall(\n",
    "            re.escape(\"[E1]\") + \".*\" + re.escape(\"[/E1]\"), ner_dataset[\"sentence\"][i]\n",
    "        )\n",
    "        # 带隙（这里temp是Tc的表示）\n",
    "        temp = re.findall(\n",
    "            re.escape(\"[E2]\") + \".*\" + re.escape(\"[/E2]\"), ner_dataset[\"sentence\"][i]\n",
    "        )\n",
    "\n",
    "        if (len(comp) > 0) and (len(temp) > 0):\n",
    "            comp = comp[0].replace(\"[E1]\", \"\").replace(\"[/E1]\", \"\").replace(\" \", \"\")\n",
    "            temp = temp[0].replace(\"[E2]\", \"\").replace(\"[/E2]\", \"\").replace(\" \", \"\")\n",
    "            database[\"compound\"].append(comp)\n",
    "            database[\"Gap\"].append(temp)\n",
    "            database[\"sentence\"].append(ner_dataset[\"sentence\"][i])\n",
    "            database[\"source\"].append(ner_dataset[\"source\"][i])\n",
    "\n",
    "# The chemical entity is converted to a Composition object from pymatgen and its reduced formula is taken 从 pymatgen 将化学实体转换为组成对象，并提取其还原公式\n",
    "database = pd.DataFrame(database)\n",
    "\n",
    "valid_i = []\n",
    "for i, comp in enumerate(database[\"compound\"]):\n",
    "    try:\n",
    "        Composition(comp).get_reduced_formula_and_factor()[0]\n",
    "        valid_i.append(i)\n",
    "    except:\n",
    "        print(\n",
    "            comp, \"\\t\", database[\"sentence\"][i], \"\\n\\n\"\n",
    "        )  # The entries that raise an exception are printed for debugging purpose\n",
    "print(\"Database entries:\", len(valid_i), \"/\", len(database[\"sentence\"]))\n",
    "\n",
    "database.iloc[valid_i].to_csv(OUTPUT_CSV_3)\n",
    "\n",
    "relation_time = time.time() - start_time_relation\n",
    "total_time = time.time() - start_time_total\n",
    "\n",
    "# 记录运行时间\n",
    "with open(RUNTIME, 'w') as f:\n",
    "    f.write(f\"Classifier time: {classifier_time:.2f} seconds\\n\")\n",
    "    f.write(f\"NER time: {ner_time:.2f} seconds\\n\")\n",
    "    f.write(f\"Relation time: {relation_time:.2f} seconds\\n\")\n",
    "    f.write(f\"Total time: {total_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"postprocessing\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from my_post import clean_and_normalize, compare\n",
    "\n",
    "def post_psie(csv_2s, csv_3, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"BERT-PSIE提取的原始数据转换为标准格式\n",
    "    \n",
    "    Args:\n",
    "        csv_2s: BERT-PSIE提取的 single-mentioned 的 CSV 文件路径\n",
    "        csv_3: BERT-PSIE提取的 multiple-mentioned 的 CSV 文件路径\n",
    "        xlsx_path: 输出的Excel文件路径: \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "    \"\"\"\n",
    "    # 读取并合并CSV文件\n",
    "    df1 = pd.read_csv(csv_2s)\n",
    "    df2 = pd.read_csv(csv_3)\n",
    "    df1.columns = df2.columns\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    df = df.iloc[:, 1:]  # 去除第一列\n",
    "    df.columns = ['material', 'value', 'sentence', 'doi']  # rename\n",
    "    df = df[['sentence', 'doi', 'material', 'value']]  # reorder\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_psie(csv_2s, csv_3, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"提取流程的主函数\n",
    "    \n",
    "    Args:\n",
    "        csv_2s: BERT-PSIE提取的 single-mentioned 的 CSV 文件路径\n",
    "        csv_3: BERT-PSIE提取的 multiple-mentioned 的 CSV 文件路径\n",
    "        xlsx_path: 结果Excel - \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: 标准答案文件路径\n",
    "        code: 提取方法代码\n",
    "    \"\"\"\n",
    "    post_psie(csv_2s, csv_3, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code)\n",
    "    compare(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "OUTPUT_DIR = os.path.join(HOME, \"output\", \"2-BERT-PSIE\")\n",
    "OUTPUT_CSV_2_S = os.path.join(OUTPUT_DIR, \"2-test_extraction_single_mentions.csv\")\n",
    "OUTPUT_CSV_3 = os.path.join(OUTPUT_DIR, \"3-relations_extraction.csv\")\n",
    "\n",
    "COMPARISON_XLSX = os.path.join(HOME, \"comparison.xlsx\")\n",
    "CODE = \"PSIE\"\n",
    "XLSX_PATH = os.path.join(HOME, f\"2_{CODE}_{dtime}.xlsx\")\n",
    "\n",
    "postprocess_psie(OUTPUT_CSV_2_S, OUTPUT_CSV_3, XLSX_PATH, COMPARISON_XLSX, CODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-CE\n",
    "kernel: lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"extract\"\"\"\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "import logging\n",
    "from datetime import datetime\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def inference(MODEL, CODE, PROPERTY, TXT_DIR, OUTPUT_DIR, logger):\n",
    "    # 第一阶段：过滤句子\n",
    "    def get_positive_sentences(model_name):\n",
    "        MODEL_NAME = model_name.replace(\":\", \"-\")\n",
    "        POSITIVE_CSV = os.path.join(OUTPUT_DIR, f\"1_positive_sentences_{MODEL_NAME}_{CODE}_{dtime}.csv\")\n",
    "\n",
    "        classif_q = f'Is the following sentence related to \"{PROPERTY}\"? Answer only \"Yes\" or \"No\" without any explanation:'\n",
    "\n",
    "        # 初始化或加载已有结果（读取已保存的CSV文件，如果不存在则创建一个空的DataFrame）\n",
    "        try:\n",
    "            df_positive = pd.read_csv(POSITIVE_CSV)\n",
    "            processed_dois = set(df_positive[\"doi\"].unique())\n",
    "        except FileNotFoundError:\n",
    "            df_positive = pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"original_index\",\n",
    "                    \"positive_sentences\",\n",
    "                    \"integrated_sentences\",\n",
    "                    \"doi\",\n",
    "                ]\n",
    "            )\n",
    "            processed_dois = set()\n",
    "\n",
    "        # 初始化模型\n",
    "        llm = ChatOllama(model=model_name, temperature=0)\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"You are an expert extraction algorithm specialized in materials science.\",\n",
    "                ),\n",
    "                (\"human\", \"{question}\\n{text}\\n\"),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | llm\n",
    "\n",
    "        # 处理\n",
    "        txt_files = glob.glob(os.path.join(TXT_DIR, \"*.txt\"))\n",
    "        logger.info(\n",
    "            f\"Found {len(txt_files)} text files for processing with {model_name}\"\n",
    "        )\n",
    "\n",
    "        for txt_path in tqdm(txt_files, desc=f\"Processing {model_name}\"):\n",
    "            doi = os.path.basename(txt_path).replace(\".txt\", \"\").replace(\"_\", \"/\")\n",
    "            if doi in processed_dois:\n",
    "                continue\n",
    "\n",
    "            # 1. [TXT] -> [句子列表]\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                sentences = [line.strip() for line in f]\n",
    "\n",
    "            # 2. 判断\n",
    "            results = []\n",
    "            for idx, sentence in enumerate(sentences):\n",
    "                try:\n",
    "                    answer = chain.invoke(\n",
    "                        {\"question\": classif_q, \"text\": sentence}\n",
    "                    ).content\n",
    "                    answer = re.sub(r\"[^\\w\\s]\", \"\", answer).strip().lower()\n",
    "                    results.append((idx, sentence, 1 if answer == \"yes\" else 0))\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing sentence {idx} in {doi}: {str(e)}\")\n",
    "                    results.append((idx, sentence, 0))\n",
    "\n",
    "            # 3. 生成集成句子（Sentence_Before + Sentence_Itself）\n",
    "            positive_data = []\n",
    "            for idx, (sentence_idx, sentence, label) in enumerate(results):\n",
    "                if label == 1:\n",
    "                    integrated = (\n",
    "                        f\"{results[idx-1][1]} {sentence}\" if idx > 0 else sentence\n",
    "                    )\n",
    "                    positive_data.append(\n",
    "                        {\n",
    "                            \"original_index\": f\"{sentence_idx}/{len(sentences)}\",\n",
    "                            \"positive_sentences\": sentence,\n",
    "                            \"integrated_sentences\": integrated,\n",
    "                            \"doi\": doi,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # 定期保存\n",
    "            if positive_data:\n",
    "                df_positive = pd.concat([df_positive, pd.DataFrame(positive_data)])\n",
    "                df_positive.to_csv(POSITIVE_CSV, index=False)\n",
    "                logger.info(f\"Saved {len(positive_data)} positive sentences from {doi}\")\n",
    "\n",
    "        return POSITIVE_CSV\n",
    "\n",
    "    # 第二阶段：数据提取\n",
    "    def extract_data(model_name, csv_path):\n",
    "        TEMPERATURE = 0\n",
    "        CONTEXT = 4096  # Default: 2048\n",
    "        MODEL_NAME = model_name.replace(\":\", \"-\")\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "        # 初始化输出文件\n",
    "        EXTRACTED_CSV = os.path.join(\n",
    "            OUTPUT_DIR, f\"2_extracted_{MODEL_NAME}_{CODE}_{dtime}.csv\"\n",
    "        )\n",
    "        BINCLAS_CSV = os.path.join(OUTPUT_DIR, f\"2_binclas_{MODEL_NAME}_{CODE}_{dtime}.csv\")\n",
    "        DIALOGUE_CSV = os.path.join(OUTPUT_DIR, f\"2_dialogues_{MODEL_NAME}_{CODE}_{dtime}.csv\")\n",
    "\n",
    "        # 创建带标题的空CSV文件\n",
    "        pd.DataFrame(columns=[\n",
    "            \"passage\", \"sentence\", \"doi\", \"material\", \"value\", \"unit\",\n",
    "            \"material_valid\", \"value_valid\", \"unit_valid\"\n",
    "        ]).to_csv(EXTRACTED_CSV, index=False)\n",
    "\n",
    "        # PROMPTS\n",
    "        classif_q = f'Answer only \"Yes\" or \"No\" without any explanation. Based on the following text, is there a value of **{PROPERTY}** mentioned in it?\\n\\n'  # wk changed\n",
    "        ifmulti_q = f'Answer \"Yes\" or \"No\" only. Does the following text mention more than one value of **{PROPERTY}**?\\n\\n'  # wk changed\n",
    "        single_q = [\n",
    "            f'Give the number only without units, do not use a full sentence. If the value is not present in the text, type \"None\". What is the value of the **{PROPERTY}** in the following text?\\n\\n',\n",
    "            f'Give the unit only, do not use a full sentence. If the unit is not present in the text, type \"None\". What is the unit of the **{PROPERTY}** in the following text?\\n\\n',\n",
    "            f'Give the name of the material only, do not use a full sentence. If the name of the material is not present in the text, type \"None\". What is the material for which the **{PROPERTY}** is given in the following text?\\n\\n',\n",
    "        ]\n",
    "        singlefollowup_q = [\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                f\" the value of the **{PROPERTY}** for the material in the following text?\\n\\n\",\n",
    "            ],\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                f\" the unit of the value of **{PROPERTY}** in the following text?\\n\\n\",\n",
    "            ],\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                f\" the material for which the value of **{PROPERTY}** is given in the following text? Make sure it is a real material.\\n\\n\",\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        tab_q = f'Use only data present in the text. If data is not present in the text, type \"None\". Summarize the values of **{PROPERTY}** in the following text in a form of a table consisting of: Material, Value, Unit. Ensure that the \"Value\" and \"Unit\" are separated into different columns.\\n\\n'  # wk changed\n",
    "        tabfollowup_q = [\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                \" the \",\n",
    "                f\" material for which the value of **{PROPERTY}** is given in the following text? Make sure it is a real material.\\n\\n\",\n",
    "            ],\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                f\" the value of the **{PROPERTY}** for the \",\n",
    "                \" material in the following text?\\n\\n\",\n",
    "            ],\n",
    "            [\n",
    "                'There is a possibility that the data you extracted is incorrect. Answer \"Yes\" or \"No\" only. Be very strict. Is ',\n",
    "                \" the unit of the \",\n",
    "                f\" value of **{PROPERTY}** in the following text?\\n\\n\",\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        it = [\n",
    "            \"first\",\n",
    "            \"second\",\n",
    "            \"third\",\n",
    "            \"fourth\",\n",
    "            \"fifth\",\n",
    "            \"sixth\",\n",
    "            \"seventh\",\n",
    "            \"eighth\",\n",
    "            \"ninth\",\n",
    "            \"tenth\",\n",
    "            \"eleventh\",\n",
    "            \"twelfth\",\n",
    "            \"thirteenth\",\n",
    "            \"fourteenth\",\n",
    "            \"fifteenth\",\n",
    "            \"sixteenth\",\n",
    "            \"seventeenth\",\n",
    "            \"eighteenth\",\n",
    "            \"nineteenth\",\n",
    "            \"twentieth\",\n",
    "        ]\n",
    "        col = [\"Material\", \"Value\", \"Unit\"]\n",
    "\n",
    "        single_cols = [\"value\", \"unit\", \"material\"]\n",
    "\n",
    "        # 初始化模型\n",
    "        llm = ChatOllama(model=model_name, temperature=TEMPERATURE, num_ctx=CONTEXT)\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"You are an expert extraction algorithm specialized in materials science.\",\n",
    "                ),\n",
    "                (\"placeholder\", \"{conversation}\"),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | llm\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            ntot = len(df)\n",
    "            logger.info(f\"Starting data extraction for {len(df)} entries\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read CSV: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        with tqdm(total=ntot, desc=f\"Extracting {MODEL_NAME}\") as pbar:\n",
    "            for i in range(ntot):\n",
    "                try:\n",
    "                    binary_classif = []\n",
    "                    sss = []\n",
    "                    sss.append((\"human\", classif_q + df[\"positive_sentences\"][i]))\n",
    "                    ans = chain.invoke({\"conversation\": sss}).content\n",
    "                    sss.append((\"ai\", ans))\n",
    "                    if \"yes\" in ans.strip().lower():  # wk: positive\n",
    "                        binary_classif.append(1)\n",
    "                        result = {}\n",
    "                        passage = df[\"integrated_sentences\"][i]\n",
    "                        sentence = df[\"positive_sentences\"][i]\n",
    "                        sss.append((\"human\", ifmulti_q + passage))\n",
    "                        ans = chain.invoke({\"conversation\": sss}).content\n",
    "                        sss.append((\"ai\", ans))\n",
    "                        if \"no\" in ans.lower():  # wk: positive; single data\n",
    "                            result[\"passage\"] = [passage]\n",
    "                            result[\"sentence\"] = [sentence]\n",
    "                            result[\"doi\"] = [df[\"doi\"][i]]\n",
    "                            result[\"material\"] = []\n",
    "                            result[\"value\"] = []\n",
    "                            result[\"unit\"] = []\n",
    "                            result[\"material_valid\"] = []\n",
    "                            result[\"value_valid\"] = []\n",
    "                            result[\"unit_valid\"] = []\n",
    "                            for j in range(\n",
    "                                len(single_q)\n",
    "                            ):  # wk: iterate 'single data' questions\n",
    "                                sss.append((\"human\", single_q[j] + passage))\n",
    "                                ans = chain.invoke({\"conversation\": sss}).content\n",
    "                                sss.append((\"ai\", ans))\n",
    "                                result[single_cols[j]].append(ans)\n",
    "                                if \"none\" in ans.lower():\n",
    "                                    result[single_cols[j] + \"_valid\"].append(0)\n",
    "                                else:\n",
    "                                    result[single_cols[j] + \"_valid\"].append(1)\n",
    "                        elif \"yes\" in ans.lower():  # wk: positive; multiple data\n",
    "                            sss.append((\"human\", tab_q + passage))\n",
    "                            tab = chain.invoke({\"conversation\": sss}).content\n",
    "                            sss.append((\"ai\", tab))\n",
    "                            sst = copy(sss)\n",
    "                            tab = [\n",
    "                                split(\"[,|]\", row) for row in tab.strip().split(\"\\n\")\n",
    "                            ]\n",
    "                            tab = [\n",
    "                                [item.strip() for item in row if len(item.strip()) > 0]\n",
    "                                for row in tab\n",
    "                                if len(row) >= 3\n",
    "                            ]\n",
    "                            if len(tab) <= 0:\n",
    "                                tab.append([\"Material\", \"Value\", \"Unit\"])\n",
    "                            if len(tab) <= 1:\n",
    "                                tab.append([\"None\", \"None\", \"None\"])\n",
    "                            else:\n",
    "                                tab.pop(1)\n",
    "                            head = tab.pop(0)\n",
    "                            tab = pd.DataFrame(\n",
    "                                tab, columns=head\n",
    "                            )  # wk: change (markdown?) format table to DataFrame\n",
    "                            result[\"passage\"] = []\n",
    "                            result[\"sentence\"] = []\n",
    "                            result[\"doi\"] = []\n",
    "                            result[\"material\"] = []\n",
    "                            result[\"value\"] = []\n",
    "                            result[\"unit\"] = []\n",
    "                            result[\"material_valid\"] = []\n",
    "                            result[\"value_valid\"] = []\n",
    "                            result[\"unit_valid\"] = []\n",
    "                            for k in range(len(tab)):\n",
    "                                sst.append(\n",
    "                                    (\n",
    "                                        \"tab\",\n",
    "                                        tab[col[0]][k]\n",
    "                                        + \",\"\n",
    "                                        + tab[col[1]][k]\n",
    "                                        + \",\"\n",
    "                                        + tab[col[2]][k],\n",
    "                                    )\n",
    "                                )\n",
    "                                result[\"passage\"].append(passage)\n",
    "                                result[\"sentence\"].append(sentence)\n",
    "                                result[\"doi\"].append(df[\"doi\"][i])\n",
    "                                multi_valid = True\n",
    "                                for l in range(3):\n",
    "                                    ss = (\n",
    "                                        tabfollowup_q[l][0]\n",
    "                                        + str(tab[col[l]][k])\n",
    "                                        + tabfollowup_q[l][1]\n",
    "                                        + it[k]\n",
    "                                        + tabfollowup_q[l][2]\n",
    "                                        + passage\n",
    "                                    )\n",
    "                                    result[col[l].lower()].append(tab[col[l]][k])\n",
    "                                    if \"none\" in tab[col[l]][k].lower():\n",
    "                                        result[col[l].lower() + \"_valid\"].append(0)\n",
    "                                        multi_valid = False\n",
    "                                    elif multi_valid:\n",
    "                                        sss.append((\"human\", ss))\n",
    "                                        sst.append((\"human\", ss))\n",
    "                                        ans = chain.invoke(\n",
    "                                            {\"conversation\": sss}\n",
    "                                        ).content\n",
    "                                        sss.append((\"ai\", ans))\n",
    "                                        sst.append((\"ai\", ans))\n",
    "                                        if \"no\" in ans.lower():\n",
    "                                            result[col[l].lower() + \"_valid\"].append(0)\n",
    "                                            multi_valid = False\n",
    "                                        else:\n",
    "                                            result[col[l].lower() + \"_valid\"].append(1)\n",
    "                                    else:\n",
    "                                        result[col[l].lower() + \"_valid\"].append(1)\n",
    "                        try:  # wk: [OUTPUT1]\n",
    "                            pd.DataFrame(result).to_csv(\n",
    "                                EXTRACTED_CSV, mode=\"a\", index=False, header=False\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(\"Appending extracted gone wrong: \", i, \"  \", e)\n",
    "                            print(\"Appending extracted gone wrong: \", result, \"  \", e)\n",
    "                            print(\"Appending extracted gone wrong: \", tab, \"  \", e)\n",
    "                    else:  # wk: negative\n",
    "                        binary_classif.append(0)\n",
    "                    pd.DataFrame(binary_classif).to_csv(\n",
    "                        BINCLAS_CSV, mode=\"a\", index=False, header=False\n",
    "                    )  # wk: [OUTPUT2] positive or negative\n",
    "                    try:  # wk: [OUTPUT3] save all the conversations with GPT (save sst, if sst exsist)\n",
    "                        pd.DataFrame(sst).to_csv(\n",
    "                            DIALOGUE_CSV, mode=\"a\", index=False, header=False\n",
    "                        )\n",
    "                        del sst\n",
    "                    except:\n",
    "                        pd.DataFrame(sss).to_csv(\n",
    "                            DIALOGUE_CSV, mode=\"a\", index=False, header=False\n",
    "                        )\n",
    "                    pbar.update(1)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row {i}: {str(e)}\")\n",
    "                    print(f\"Ignoring {i+1}/{ntot} ({round(i/ntot*100,1)} %)\")\n",
    "                    continue\n",
    "\n",
    "        logger.info(f\"Completed data extraction for {model_name}\")\n",
    "        return EXTRACTED_CSV\n",
    "\n",
    "    # 主执行流程\n",
    "    stage1_output = get_positive_sentences(MODEL)\n",
    "    EXTRACTED_CSV = extract_data(MODEL, stage1_output)\n",
    "    return EXTRACTED_CSV\n",
    "\n",
    "def process_chat(MODELS, PROPERTY, TXT_DIR, OUTPUT_DIR):\n",
    "    # 初始化日志配置\n",
    "    start_time = time.time()\n",
    "    log_file = os.path.join(OUTPUT_DIR, f\"processing_{dtime}.log\")\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    # 文件日志处理器\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # 控制台日志处理器\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    logger.info(\"Initializing inference pipeline...\")\n",
    "    logger.info(f\"Property: {PROPERTY}\")\n",
    "    logger.info(f\"Input TXT directory: {TXT_DIR}\")\n",
    "    logger.info(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    for MODEL, CODE in MODELS.items():\n",
    "        logger.info(f\"\\n{'='*40}\")\n",
    "        logger.info(f\"Processing model: {MODEL}\")\n",
    "        inference(MODEL, CODE, PROPERTY, TXT_DIR, OUTPUT_DIR, logger)\n",
    "        logger.info(f\"Completed processing for {MODEL}\")\n",
    "\n",
    "    logger.info(f\"\\n{'='*40}\")\n",
    "    logger.info(f\"Total processing time: {time.time()-start_time:.2f} seconds\")\n",
    "    logger.info(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    logger.info(\"Processing complete!\")\n",
    "\n",
    "\n",
    "# 执行\n",
    "TXT_DIR = os.path.join(HOME, \"TXT(fromPDF_processed)\")\n",
    "OUTPUT_DIR = os.path.join(HOME, \"output\", \"3-ChatExtract\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MODELS = {\n",
    "    \"llama2:13b\": \"CE_1\",\n",
    "    \"llama3.1:70b\": \"CE_2\",\n",
    "    \"qwen2.5:14b\": \"CE_3\",\n",
    "}\n",
    "\n",
    "process_chat(\n",
    "    MODELS=MODELS,\n",
    "    PROPERTY=\"band gap\",\n",
    "    TXT_DIR=TXT_DIR,\n",
    "    OUTPUT_DIR=OUTPUT_DIR,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"postprocessing\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from my_post import clean_and_normalize, compare\n",
    "\n",
    "def post_ce(extracted_csv, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"ChatExtract提取的原始数据转换为标准格式\n",
    "    \n",
    "    Args:\n",
    "        extracted_csv: \"extracted_{model}_{dtime}.csv\"\n",
    "        xlsx_path: 输出的Excel文件路径: \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "    \"\"\"\n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(extracted_csv)\n",
    "    # 筛选material_valid、value_valid和unit_valid均为1的行\n",
    "    df = df[(df[\"material_valid\"] == 1) &\n",
    "            (df[\"value_valid\"] == 1) &\n",
    "            (df[\"unit_valid\"] == 1)\n",
    "            ]\n",
    "    # 去除value列中的\"meV\"或\"eV\"\n",
    "    df[\"value\"] = df[\"value\"].apply(lambda x: x.replace(\"meV\", \"\").replace(\"eV\", \"\").strip() if pd.notna(x) else x)\n",
    "    # 删去不要的列并重新排序\n",
    "    columns_to_keep = [\"doi\", \"material\", \"value\", \"unit\", \"passage\", \"sentence\"]\n",
    "    df = df[columns_to_keep]\n",
    "    # 清洗非法控制字符的函数\n",
    "    def clean_illegal_chars(text):\n",
    "        if isinstance(text, str):\n",
    "            # 移除控制字符（保留制表符\\t、换行符\\n、回车符\\r）\n",
    "            return re.sub(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F-\\x9F]\", \"\", text)\n",
    "        else:\n",
    "            return text\n",
    "    df = df.map(clean_illegal_chars)  # 应用清洗函数到整个DataFrame\n",
    "\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_ce(extracted_csv, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"CDE提取流程的主函数\n",
    "    \n",
    "    Args:\n",
    "        extracted_csv: \"extracted_{model}_{dtime}.csv\"\n",
    "        xlsx_path: 结果Excel - \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: 标准答案文件路径\n",
    "        code: 提取方法代码\n",
    "    \"\"\"\n",
    "    post_ce(extracted_csv, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code)\n",
    "    compare(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "COMPARISON_XLSX = os.path.join(HOME, \"comparison.xlsx\")\n",
    "\n",
    "DATA_DIR = OUTPUT_DIR\n",
    "CSVS = [f'2_extracted_llama2-13b_CE_1_{dtime}.csv',\n",
    "        f'2_extracted_llama3.1-70b_CE_2_{dtime}.csv',\n",
    "        f'2_extracted_qwen2.5-14b_CE_3_{dtime}.csv']\n",
    "CODES = [\"CE_1\",  # llama2:13b\n",
    "         \"CE_2\",  # llama3.1:70b\n",
    "         \"CE_3\",  # qwen2.5:14b\n",
    "         ]\n",
    "for csv, code in zip(CSVS, CODES):\n",
    "    extracted_csv = os.path.join(DATA_DIR, csv)\n",
    "    xlsx_path = os.path.join(HOME, f\"3_{code}_{dtime}.xlsx\")\n",
    "    postprocess_ce(extracted_csv, xlsx_path, COMPARISON_XLSX, code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-LC\n",
    "kernel: lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"extract\"\"\"\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "\n",
    "def process_single_pdf(pdf_file, embeddings, llm, text_splitter, prompt, question, client):\n",
    "    \"\"\"处理单个PDF文件并提取信息\n",
    "    \n",
    "    Args:\n",
    "        pdf_file: PDF文件路径\n",
    "        embeddings: 嵌入模型\n",
    "        llm: 语言模型\n",
    "        text_splitter: 文本分割器\n",
    "        prompt: 提示模板\n",
    "        question: 查询问题\n",
    "        client: ChromaDB客户端\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (doi, output, status)\n",
    "    \"\"\"\n",
    "    doi = os.path.basename(pdf_file).replace('.pdf', '').replace('_', '/')\n",
    "    try:\n",
    "        collection_name = f\"collection_{doi.replace('/', '_')}\"  # 用DOI生成唯一名称\n",
    "        try:  # 强制删除旧集合\n",
    "            client.delete_collection(collection_name)\n",
    "        except:\n",
    "            pass\n",
    "        # 加载和分割文档\n",
    "        loader = PyMuPDFLoader(file_path=pdf_file)\n",
    "        docs = loader.load()\n",
    "        chunks = text_splitter.split_documents(docs)\n",
    "        # 创建向量数据库\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            collection_name=collection_name,\n",
    "            client=client\n",
    "        )\n",
    "        # 构建和执行RAG链\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        output = rag_chain.invoke(question).content\n",
    "        status = 1 if output.strip().startswith(\"|\") else 0\n",
    "\n",
    "        # 清理资源\n",
    "        del loader, docs, chunks, vector_db, retriever, rag_chain\n",
    "        gc.collect()\n",
    "        \n",
    "        return doi, output, status\n",
    "\n",
    "    except Exception as e:\n",
    "        return doi, f\"处理错误: {str(e)}\", -1\n",
    "\n",
    "def process_pdfs_batch(codes, embed_stream, infer_stream, output_dir, pdf_dir, template, question):\n",
    "    \"\"\"批量处理PDF文件\n",
    "    \n",
    "    Args:\n",
    "        codes: 处理代码列表\n",
    "        embed_stream: 模型配对列表（每个嵌入模型配对所有推理模型）\n",
    "        infer_stream: 模型配对列表（每个嵌入模型配对所有推理模型）\n",
    "        output_dir: 输出目录\n",
    "        pdf_dir: PDF文件目录\n",
    "        template: 提示模板\n",
    "        question: 查询问题\n",
    "    \"\"\"\n",
    "    # 创建全局ChromaDB客户端\n",
    "    client_settings = Settings(persist_directory=\"\")\n",
    "    global_client = Client(client_settings)  # 使用内存数据库，避免文件残留；关键：禁用持久化\n",
    "    \n",
    "    total_start_time = time.time()  # log\n",
    "    for code, embed_model, infer_model in zip(codes, embed_stream, infer_stream):\n",
    "        output_csv = os.path.join(output_dir, f\"output_{code}_{dtime}.csv\")\n",
    "        # 初始化模型和工具\n",
    "        embeddings = OllamaEmbeddings(model=embed_model)\n",
    "        llm = ChatOllama(model=infer_model, temperature=0, num_predict=80)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "        # 处理每个PDF文件\n",
    "        pdf_files = sorted(glob.glob(os.path.join(pdf_dir, \"*.pdf\")))\n",
    "        start_time = time.time()  # log\n",
    "        for pdf_file in tqdm(pdf_files, desc=f\"处理PDF文件 - {code}\"):\n",
    "            doi, output, status = process_single_pdf(\n",
    "                pdf_file, embeddings, llm, text_splitter, prompt, question, global_client\n",
    "            )\n",
    "            # 保存结果\n",
    "            df_new = pd.DataFrame([{'doi': doi, 'output': output, 'status': status}])\n",
    "            if os.path.exists(output_csv):\n",
    "                df_new.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                df_new.to_csv(output_csv, index=False)\n",
    "        end_time = time.time()  # 结束计时\n",
    "        batch_time = end_time - start_time\n",
    "        # 写入日志文件\n",
    "        with open(log_file, 'a') as lf:\n",
    "            log_entry = (\n",
    "                f\"Code: {code} | Embed Model: {embed_model} | Infer Model: {infer_model}\\n\"\n",
    "                f\"Start: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))} | \"\n",
    "                f\"End: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))} | \"\n",
    "                f\"Duration: {batch_time:.2f} seconds | Status: {'Success' if status == 1 else 'Error'}\\n\"\n",
    "                f\"{'='*40}\\n\"\n",
    "            )\n",
    "            lf.write(log_entry)\n",
    "    total_end_time = time.time()  # 记录总结束时间\n",
    "    total_time = total_end_time - total_start_time\n",
    "    with open(log_file, 'a') as lf:\n",
    "        log_entry = (\n",
    "            f\"Total Start: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time))}\\n\"\n",
    "            f\"Total End: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_end_time))}\\n\"\n",
    "            f\"Total Duration: {total_time:.2f} seconds\\n\"\n",
    "            f\"{'='*40}\\n\"\n",
    "        )\n",
    "        lf.write(log_entry)\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "You are an expert information extraction algorithm.\n",
    "Extract all the band gap values in the CONTEXT given blow.\n",
    "Output the band gap values in the form of a markdown table, including: Material (name of the material), Value (band gap value), Unit (unit of value).\n",
    "Do not explain, only output the table in markdown format.\n",
    "The output is strictly in the following format.\n",
    "| Material | Value | Unit |\n",
    "|----------|-------|------|\n",
    "| ... | ... | eV |\n",
    "| ... | ... | meV |\n",
    "If no band gap values mentioned in the article, the following table is acceptable:\n",
    "| Material | Value | Unit |\n",
    "|----------|-------|------|\n",
    "| None | None | None |\n",
    "---\n",
    "CONTEXT: {context}\n",
    "---\n",
    "QUESTION: {question}\n",
    "Answer in markdown table:\n",
    "\"\"\"\n",
    "QUESTION = \"What are the materials' name and their band gap values?\"\n",
    "\n",
    "TXT_DIR = os.path.join(HOME, \"TXT(fromPDF_processed)\")\n",
    "PDF_DIR = os.path.join(HOME, \"PDF\")\n",
    "OUTPUT_DIR = os.path.join(HOME, \"output\", \"4-LangChain\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "log_file = os.path.join(OUTPUT_DIR, f\"log_{dtime}.log\")\n",
    "EMBEDDING_MODELS = [\n",
    "    \"nomic-embed-text\",\n",
    "    \"bge-m3\",\n",
    "    ]\n",
    "INFERENCE_MODELS = [\n",
    "    \"llama2:13b\",\n",
    "    \"llama3.1:70b\",\n",
    "    \"qwen2.5:14b\",\n",
    "    ]\n",
    "CODES = [f\"LC_{i+1}{j+1}\" \n",
    "        for i in range(len(EMBEDDING_MODELS)) \n",
    "        for j in range(len(INFERENCE_MODELS))]\n",
    "embed_stream = [model for model in EMBEDDING_MODELS for _ in INFERENCE_MODELS]\n",
    "infer_stream = INFERENCE_MODELS * len(EMBEDDING_MODELS)\n",
    "\n",
    "# 执行批处理\n",
    "process_pdfs_batch(\n",
    "    CODES,\n",
    "    embed_stream,\n",
    "    infer_stream,\n",
    "    OUTPUT_DIR,\n",
    "    PDF_DIR,\n",
    "    TEMPLATE,\n",
    "    QUESTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"postprocessing\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from my_post import clean_and_normalize, compare\n",
    "\n",
    "def post_lc(extracted_csv, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"将LangChain提取的原始数据转换为标准格式\n",
    "    \n",
    "    将CSV格式的原始数据转换为标准的Excel格式，包含doi、material和value三列。\n",
    "    处理过程包括:\n",
    "    1. 读取并去重原始数据\n",
    "    2. 解析每行中的markdown表格\n",
    "    3. 标准化表格格式并合并结果\n",
    "    \n",
    "    Args:\n",
    "        extracted_csv (str): 输入的CSV文件路径，格式为\"output_{embed}_{infer}_{dtime}.csv\"\n",
    "        xlsx_path (str): 输出的Excel文件路径，格式为\"FINAL_{embed}_{infer}_{dtime}.xlsx\"\n",
    "    \"\"\"\n",
    "    # 读取CSV并去重\n",
    "    raw_df = pd.read_csv(extracted_csv)\n",
    "    raw_df = raw_df.drop_duplicates(subset=[\"output\"], keep=\"first\")\n",
    "    # 只保留status为1的行\n",
    "    raw_df = raw_df[raw_df['status'] == 1]\n",
    "    # 初始化结果DataFrame\n",
    "    result_df = pd.DataFrame(columns=[\"doi\", \"material\", \"value\", \"unit\"])\n",
    "    \n",
    "    def parse_markdown_table(output, doi):\n",
    "        \"\"\"解析单个markdown表格\n",
    "        \n",
    "        Args:\n",
    "            output (str): markdown格式的表格字符串\n",
    "            doi (str): 对应的DOI\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: 解析后的表格，若解析失败返回None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 解析markdown表格\n",
    "            table = pd.read_csv(StringIO(output), sep=\"|\", skipinitialspace=True)\n",
    "            table = table.dropna(axis=1, how=\"all\")  # 删除空列\n",
    "            table = table.iloc[1:, :]  # 删除表头行\n",
    "            \n",
    "            # 验证并格式化表格\n",
    "            if len(table.columns) == 3:\n",
    "                table.columns = [\"material\", \"value\", \"unit\"]\n",
    "                table[\"material\"] = table[\"material\"].str.strip()\n",
    "                table[\"doi\"] = doi\n",
    "                return table[[\"doi\", \"material\", \"value\", \"unit\"]]\n",
    "            return None\n",
    "        except Exception:\n",
    "            print(f\"无法处理DOI为{doi}的表格\")\n",
    "            return None\n",
    "    \n",
    "    # 处理每一行数据\n",
    "    parsed_tables = []\n",
    "    for _, row in raw_df.iterrows():\n",
    "        table = parse_markdown_table(row[\"output\"], row[\"doi\"])\n",
    "        if table is not None:\n",
    "            parsed_tables.append(table)\n",
    "    # 合并、保存所有解析结果\n",
    "    if parsed_tables:\n",
    "        result_df = pd.concat(parsed_tables, ignore_index=True)\n",
    "    \n",
    "    \"\"\"简单处理\"\"\"\n",
    "    # 去除value列中的\"meV\"或\"eV\"\n",
    "    result_df[\"value\"] = result_df[\"value\"].apply(lambda x: x.replace(\"meV\", \"\").replace(\"eV\", \"\").strip() if pd.notna(x) else x)\n",
    "    # 清洗非法控制字符的函数\n",
    "    def clean_illegal_chars(text):\n",
    "        if isinstance(text, str):\n",
    "            # 移除控制字符（保留制表符\\t、换行符\\n、回车符\\r）\n",
    "            return re.sub(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F-\\x9F]\", \"\", text)\n",
    "        else:\n",
    "            return text\n",
    "    result_df = result_df.map(clean_illegal_chars)  # 应用清洗函数到整个DataFrame\n",
    "    \n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=\"0-raw\", index=False)\n",
    "    \n",
    "    # 删去空值\n",
    "    result_df = result_df[~result_df[\"value\"].str.lower().str.strip().isin([\"none\", \"nan\"]) & result_df[\"value\"].notna()]\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_lc(extracted_csv, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"CDE提取流程的主函数\n",
    "    \n",
    "    Args:\n",
    "        extracted_csv: \"extracted_{model}_{dtime}.csv\"\n",
    "        xlsx_path: 结果Excel - \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: 标准答案文件路径\n",
    "        code: 提取方法代码\n",
    "    \"\"\"\n",
    "    post_lc(extracted_csv, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code)\n",
    "    compare(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "COMPARISON_XLSX = os.path.join(HOME, \"comparison.xlsx\")\n",
    "\n",
    "DATA_DIR = OUTPUT_DIR\n",
    "csv_files = glob.glob(os.path.join(DATA_DIR, \"output_LC_*.csv\"))\n",
    "csv_files.sort()\n",
    "# print(csv_files)\n",
    "codes = [f\"{os.path.basename(file).split('_')[1]}_{os.path.basename(file).split('_')[2]}\" for file in csv_files]\n",
    "\n",
    "for extracted_csv, code in zip(csv_files, codes):\n",
    "    print(extracted_csv, code)\n",
    "    xlsx_path = os.path.join(HOME, f\"4_{code}_{dtime}.xlsx\")\n",
    "    postprocess_lc(extracted_csv, xlsx_path, COMPARISON_XLSX, code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Kimi\n",
    "kernel: lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"postprocessing\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from my_post import clean_and_normalize, compare\n",
    "\n",
    "def post_kimi(extracted_xlsx, xlsx_path, comparison_xlsx, code):\n",
    "    df = pd.read_excel(extracted_xlsx, sheet_name=\"1-Markdown\")\n",
    "    result_df = pd.DataFrame(columns=[\"doi\", \"material\", \"value\", \"sentence\"])\n",
    "    # 遍历'output'列\n",
    "    for index, row in df.iterrows():\n",
    "        doi = row[\"doi\"].replace('_', '/')\n",
    "        output = row[\"output\"]\n",
    "        # 手动处理每行，避免内容中的竖线被误分割\n",
    "        lines = [line.strip() for line in output.split('\\n') if line.strip()]\n",
    "        # 提取表头\n",
    "        headers = [h.strip() for h in lines[0].strip('| ').split('|')]\n",
    "        headers = [h for h in headers if h]  # 去除空字符串\n",
    "        # 处理数据行（跳过第二行的分隔线）\n",
    "        data = []\n",
    "        for line in lines[2:]:\n",
    "            # 去除行首尾的管道符并按列分割\n",
    "            stripped_line = line.strip('| ')\n",
    "            parts = [p.strip() for p in stripped_line.split('|')]\n",
    "            # 确保每行有三列（Material, Value, Sentence）\n",
    "            if len(parts) == 3:\n",
    "                data.append(parts)\n",
    "            else:\n",
    "                # 处理异常行（例如内容中有竖线但被错误分割）\n",
    "                # 这里简单示例，实际需根据数据调整\n",
    "                material = parts[0].strip()\n",
    "                value = parts[1].strip()\n",
    "                sentence = '|'.join(parts[2:]).strip()  # 合并多余部分\n",
    "                data.append([material, value, sentence])\n",
    "        # 创建DataFrame\n",
    "        table = pd.DataFrame(data, columns=headers)\n",
    "        # 删除空白列和标题行\n",
    "        table = table.dropna(axis=1, how=\"all\").iloc[1:]\n",
    "        # 确保表格为三列\n",
    "        if len(table.columns) == 3:\n",
    "            # 重命名列\n",
    "            table.columns = [\"material\", \"value\", \"sentence\"]\n",
    "            # 添加doi列\n",
    "            table[\"doi\"] = doi\n",
    "            # 调整列顺序\n",
    "            table = table[[\"doi\", \"material\", \"value\", \"sentence\"]]\n",
    "            # 将处理后的表格添加到结果DataFrame\n",
    "            result_df = pd.concat([result_df, table], ignore_index=True)\n",
    "    # 保存结果\n",
    "    with pd.ExcelWriter(xlsx_path, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=\"1-raw\", index=False)\n",
    "    with pd.ExcelWriter(comparison_xlsx, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "        result_df.to_excel(writer, sheet_name=f\"{code}_raw\", index=False)\n",
    "\n",
    "def postprocess_kimi(extracted_xlsx, xlsx_path, comparison_xlsx, code):\n",
    "    \"\"\"CDE提取流程的主函数\n",
    "    \n",
    "    Args:\n",
    "        xlsx_path: 结果Excel - \"FINAL_{CODE}_{dtime}.xlsx\"\n",
    "        comparison_xlsx: 标准答案文件路径\n",
    "        code: 提取方法代码\n",
    "    \"\"\"\n",
    "    post_kimi(extracted_xlsx, xlsx_path, comparison_xlsx, code)\n",
    "    clean_and_normalize(xlsx_path, comparison_xlsx, code)\n",
    "    compare(xlsx_path, comparison_xlsx, code)\n",
    "\n",
    "dtime = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "COMPARISON_XLSX = os.path.join(HOME, \"comparison.xlsx\")\n",
    "CODE = \"Kimi\"\n",
    "KIMI_OUT = os.path.join(HOME, \"output\", \"5-Kimi\", 'kimi.20250217221417285.xlsx')\n",
    "xlsx_path = os.path.join(HOME, f\"5_{CODE}_{dtime}.xlsx\")\n",
    "postprocess_kimi(KIMI_OUT, xlsx_path, COMPARISON_XLSX, CODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "COMPARISON_XLSX = os.path.join(HOME, \"comparison.xlsx\")\n",
    "\n",
    "df = pd.read_excel(COMPARISON_XLSX, sheet_name=\"summary\")\n",
    "# 删去manual之外的内容\n",
    "df.dropna(subset=['material'], inplace=True)\n",
    "df.drop(columns=[\"other_mat\"], inplace=True)\n",
    "\n",
    "# 读取error之后的所有列的内容\n",
    "methods = df.columns[df.columns.str.contains('_error')].str.replace('_error', '').unique()  # 获取所有方法名\n",
    "# 删去所有的error列\n",
    "df = df.loc[:, ~df.columns.str.contains('_error')]\n",
    "for method in methods:\n",
    "    method_col = f\"{method}\"\n",
    "    # 如果f“{method}”列某一行不为空，则替换为1\n",
    "    df[method_col] = df[method_col].apply(lambda x: 1 if pd.notna(x) else x)\n",
    "\n",
    "# 保存处理后的表格在同一个xlsx文件下的新sheet“PRF”\n",
    "with pd.ExcelWriter(COMPARISON_XLSX, mode=\"a\", if_sheet_exists=\"replace\", engine=\"openpyxl\") as writer:\n",
    "    df.to_excel(writer, sheet_name=\"PRF\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
